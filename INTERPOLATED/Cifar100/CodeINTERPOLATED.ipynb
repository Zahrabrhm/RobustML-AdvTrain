{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n06ETa8tDjGI"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "sys.argv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import numpy as np\n",
    "SEED=1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic=True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5GOdCqYCD1v3"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('model-resnet-epoch76.pt'):\n",
    "    os.makedirs('model-resnet-epoch76.pt')\n",
    "use_cuda = not False and torch.cuda.is_available()\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpv_FA8aD3bH",
    "outputId": "ff0259db-63a6-4ffa-f4f1-4e2d7fdcad79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=ResNet18()\n",
    "net.load_state_dict(torch.load('model-resnet-epoch150.pt'))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34OWES3ID5jn",
    "outputId": "8416a20c-fa18-4969-b097-970b95205c2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "[ Train epoch: 0 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 3.6272807121276855\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 9.249380111694336\n",
      "\n",
      "Current batch: 10\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 3.0009193420410156\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 6.971699237823486\n",
      "\n",
      "Current batch: 20\n",
      "Current benign train accuracy: 0.1015625\n",
      "Current benign train loss: 4.330103874206543\n",
      "Current adversarial train accuracy: 0.046875\n",
      "Current adversarial train loss: 4.56636381149292\n",
      "\n",
      "Current batch: 30\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 3.991178035736084\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 4.353630065917969\n",
      "\n",
      "Current batch: 40\n",
      "Current benign train accuracy: 0.1953125\n",
      "Current benign train loss: 3.6357054710388184\n",
      "Current adversarial train accuracy: 0.0546875\n",
      "Current adversarial train loss: 4.344147682189941\n",
      "\n",
      "Current batch: 50\n",
      "Current benign train accuracy: 0.21875\n",
      "Current benign train loss: 3.7239980697631836\n",
      "Current adversarial train accuracy: 0.0859375\n",
      "Current adversarial train loss: 4.3727641105651855\n",
      "\n",
      "Current batch: 60\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 3.533858060836792\n",
      "Current adversarial train accuracy: 0.09375\n",
      "Current adversarial train loss: 4.278804779052734\n",
      "\n",
      "Current batch: 70\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 3.902006149291992\n",
      "Current adversarial train accuracy: 0.0625\n",
      "Current adversarial train loss: 4.398541450500488\n",
      "\n",
      "Current batch: 80\n",
      "Current benign train accuracy: 0.359375\n",
      "Current benign train loss: 3.0200998783111572\n",
      "Current adversarial train accuracy: 0.0546875\n",
      "Current adversarial train loss: 4.432266712188721\n",
      "\n",
      "Current batch: 90\n",
      "Current benign train accuracy: 0.3828125\n",
      "Current benign train loss: 2.830368995666504\n",
      "Current adversarial train accuracy: 0.0546875\n",
      "Current adversarial train loss: 4.367166519165039\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 3.692218542098999\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 4.167669296264648\n",
      "\n",
      "Current batch: 110\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.5495119094848633\n",
      "Current adversarial train accuracy: 0.078125\n",
      "Current adversarial train loss: 4.231559753417969\n",
      "\n",
      "Current batch: 120\n",
      "Current benign train accuracy: 0.453125\n",
      "Current benign train loss: 2.476206064224243\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 4.217702388763428\n",
      "\n",
      "Current batch: 130\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.4234793186187744\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 4.1528706550598145\n",
      "\n",
      "Current batch: 140\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 3.112997531890869\n",
      "Current adversarial train accuracy: 0.078125\n",
      "Current adversarial train loss: 4.330543518066406\n",
      "\n",
      "Current batch: 150\n",
      "Current benign train accuracy: 0.3203125\n",
      "Current benign train loss: 3.566784381866455\n",
      "Current adversarial train accuracy: 0.1328125\n",
      "Current adversarial train loss: 4.025376319885254\n",
      "\n",
      "Current batch: 160\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 3.5978965759277344\n",
      "Current adversarial train accuracy: 0.1484375\n",
      "Current adversarial train loss: 4.123743534088135\n",
      "\n",
      "Current batch: 170\n",
      "Current benign train accuracy: 0.515625\n",
      "Current benign train loss: 2.4977965354919434\n",
      "Current adversarial train accuracy: 0.046875\n",
      "Current adversarial train loss: 4.240024089813232\n",
      "\n",
      "Current batch: 180\n",
      "Current benign train accuracy: 0.125\n",
      "Current benign train loss: 3.5661611557006836\n",
      "Current adversarial train accuracy: 0.1171875\n",
      "Current adversarial train loss: 4.098767280578613\n",
      "\n",
      "Current batch: 190\n",
      "Current benign train accuracy: 0.5546875\n",
      "Current benign train loss: 2.173906087875366\n",
      "Current adversarial train accuracy: 0.140625\n",
      "Current adversarial train loss: 3.9748337268829346\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.09375\n",
      "Current benign train loss: 3.578730583190918\n",
      "Current adversarial train accuracy: 0.140625\n",
      "Current adversarial train loss: 4.07605504989624\n",
      "\n",
      "Current batch: 210\n",
      "Current benign train accuracy: 0.4921875\n",
      "Current benign train loss: 3.069399833679199\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.8037338256835938\n",
      "\n",
      "Current batch: 220\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.3952584266662598\n",
      "Current adversarial train accuracy: 0.15625\n",
      "Current adversarial train loss: 3.8347737789154053\n",
      "\n",
      "Current batch: 230\n",
      "Current benign train accuracy: 0.09375\n",
      "Current benign train loss: 3.765012741088867\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 4.118593215942383\n",
      "\n",
      "Current batch: 240\n",
      "Current benign train accuracy: 0.3984375\n",
      "Current benign train loss: 3.2786052227020264\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 4.0528645515441895\n",
      "\n",
      "Current batch: 250\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 3.5556464195251465\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 4.196495532989502\n",
      "\n",
      "Current batch: 260\n",
      "Current benign train accuracy: 0.3359375\n",
      "Current benign train loss: 3.43131947517395\n",
      "Current adversarial train accuracy: 0.1015625\n",
      "Current adversarial train loss: 4.3292555809021\n",
      "\n",
      "Current batch: 270\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.8803796768188477\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.8057990074157715\n",
      "\n",
      "Current batch: 280\n",
      "Current benign train accuracy: 0.4296875\n",
      "Current benign train loss: 3.206284999847412\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 4.011422157287598\n",
      "\n",
      "Current batch: 290\n",
      "Current benign train accuracy: 0.1484375\n",
      "Current benign train loss: 3.8101272583007812\n",
      "Current adversarial train accuracy: 0.1015625\n",
      "Current adversarial train loss: 4.129193305969238\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.078125\n",
      "Current benign train loss: 3.4925692081451416\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.9002726078033447\n",
      "\n",
      "Current batch: 310\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 2.2147645950317383\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.846463203430176\n",
      "\n",
      "Current batch: 320\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.396223545074463\n",
      "Current adversarial train accuracy: 0.109375\n",
      "Current adversarial train loss: 4.2991485595703125\n",
      "\n",
      "Current batch: 330\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.5334296226501465\n",
      "Current adversarial train accuracy: 0.1015625\n",
      "Current adversarial train loss: 4.334442138671875\n",
      "\n",
      "Current batch: 340\n",
      "Current benign train accuracy: 0.1796875\n",
      "Current benign train loss: 3.71472430229187\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.758287191390991\n",
      "\n",
      "Current batch: 350\n",
      "Current benign train accuracy: 0.46875\n",
      "Current benign train loss: 3.1396565437316895\n",
      "Current adversarial train accuracy: 0.125\n",
      "Current adversarial train loss: 4.04575777053833\n",
      "\n",
      "Current batch: 360\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.0502817630767822\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.8539605140686035\n",
      "\n",
      "Current batch: 370\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.5193634033203125\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.747730255126953\n",
      "\n",
      "Current batch: 380\n",
      "Current benign train accuracy: 0.3359375\n",
      "Current benign train loss: 3.596283197402954\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.88992977142334\n",
      "\n",
      "Current batch: 390\n",
      "Current benign train accuracy: 0.6625\n",
      "Current benign train loss: 2.3012619018554688\n",
      "Current adversarial train accuracy: 0.175\n",
      "Current adversarial train loss: 3.8910772800445557\n",
      "\n",
      "Total benign train accuarcy: tensor(33.9609, device='cuda:0')\n",
      "Total adversarial train accuarcy: tensor(10.0192, device='cuda:0')\n",
      "Total benign train loss: 1216.3123574256897\n",
      "Total adversarial train loss: 1701.7644295692444\n",
      "\n",
      "[ Test epoch: 0 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign test accuracy: 0.64\n",
      "Current benign test loss: 1.754259467124939\n",
      "Current adversarial test accuracy: 0.14\n",
      "Current adversarial test loss: 4.001382827758789\n",
      "\n",
      "Current batch: 10\n",
      "Current benign test accuracy: 0.56\n",
      "Current benign test loss: 1.8969299793243408\n",
      "Current adversarial test accuracy: 0.12\n",
      "Current adversarial test loss: 4.109935283660889\n",
      "\n",
      "Current batch: 20\n",
      "Current benign test accuracy: 0.57\n",
      "Current benign test loss: 1.8122453689575195\n",
      "Current adversarial test accuracy: 0.13\n",
      "Current adversarial test loss: 4.036714553833008\n",
      "\n",
      "Current batch: 30\n",
      "Current benign test accuracy: 0.55\n",
      "Current benign test loss: 1.9752442836761475\n",
      "Current adversarial test accuracy: 0.1\n",
      "Current adversarial test loss: 4.14567232131958\n",
      "\n",
      "Current batch: 40\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.9031953811645508\n",
      "Current adversarial test accuracy: 0.16\n",
      "Current adversarial test loss: 4.10612678527832\n",
      "\n",
      "Current batch: 50\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.765853762626648\n",
      "Current adversarial test accuracy: 0.16\n",
      "Current adversarial test loss: 4.015449047088623\n",
      "\n",
      "Current batch: 60\n",
      "Current benign test accuracy: 0.56\n",
      "Current benign test loss: 1.8337364196777344\n",
      "Current adversarial test accuracy: 0.15\n",
      "Current adversarial test loss: 3.9215309619903564\n",
      "\n",
      "Current batch: 70\n",
      "Current benign test accuracy: 0.55\n",
      "Current benign test loss: 1.9400601387023926\n",
      "Current adversarial test accuracy: 0.14\n",
      "Current adversarial test loss: 4.114520072937012\n",
      "\n",
      "Current batch: 80\n",
      "Current benign test accuracy: 0.51\n",
      "Current benign test loss: 2.0890817642211914\n",
      "Current adversarial test accuracy: 0.11\n",
      "Current adversarial test loss: 4.209112167358398\n",
      "\n",
      "Current batch: 90\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.8927403688430786\n",
      "Current adversarial test accuracy: 0.13\n",
      "Current adversarial test loss: 3.988009452819824\n",
      "\n",
      "Total benign test accuarcy: 57.45\n",
      "Total adversarial test Accuarcy: 12.78\n",
      "Total benign test loss: 185.39157485961914\n",
      "Total adversarial test loss: 404.3600080013275\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 1 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.6640625\n",
      "Current benign train loss: 2.370206356048584\n",
      "Current adversarial train accuracy: 0.078125\n",
      "Current adversarial train loss: 4.031607627868652\n",
      "\n",
      "Current batch: 10\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.200101375579834\n",
      "Current adversarial train accuracy: 0.1796875\n",
      "Current adversarial train loss: 3.585505962371826\n",
      "\n",
      "Current batch: 20\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 3.2502665519714355\n",
      "Current adversarial train accuracy: 0.1953125\n",
      "Current adversarial train loss: 3.7657055854797363\n",
      "\n",
      "Current batch: 30\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.8790916204452515\n",
      "Current adversarial train accuracy: 0.109375\n",
      "Current adversarial train loss: 4.106932640075684\n",
      "\n",
      "Current batch: 40\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.5792012214660645\n",
      "Current adversarial train accuracy: 0.1484375\n",
      "Current adversarial train loss: 4.006755828857422\n",
      "\n",
      "Current batch: 50\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.5227043628692627\n",
      "Current adversarial train accuracy: 0.1328125\n",
      "Current adversarial train loss: 4.103693962097168\n",
      "\n",
      "Current batch: 60\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.2634332180023193\n",
      "Current adversarial train accuracy: 0.1484375\n",
      "Current adversarial train loss: 3.8708014488220215\n",
      "\n",
      "Current batch: 70\n",
      "Current benign train accuracy: 0.609375\n",
      "Current benign train loss: 2.581502914428711\n",
      "Current adversarial train accuracy: 0.140625\n",
      "Current adversarial train loss: 3.7305359840393066\n",
      "\n",
      "Current batch: 80\n",
      "Current benign train accuracy: 0.5390625\n",
      "Current benign train loss: 3.0905041694641113\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.9614105224609375\n",
      "\n",
      "Current batch: 90\n",
      "Current benign train accuracy: 0.671875\n",
      "Current benign train loss: 2.137678384780884\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.8825876712799072\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.203125\n",
      "Current benign train loss: 3.512965679168701\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.828636407852173\n",
      "\n",
      "Current batch: 110\n",
      "Current benign train accuracy: 0.65625\n",
      "Current benign train loss: 2.2411482334136963\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.6491971015930176\n",
      "\n",
      "Current batch: 120\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 3.4107041358947754\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 4.0451860427856445\n",
      "\n",
      "Current batch: 130\n",
      "Current benign train accuracy: 0.3984375\n",
      "Current benign train loss: 3.356886148452759\n",
      "Current adversarial train accuracy: 0.109375\n",
      "Current adversarial train loss: 4.054798126220703\n",
      "\n",
      "Current batch: 140\n",
      "Current benign train accuracy: 0.1640625\n",
      "Current benign train loss: 3.4248881340026855\n",
      "Current adversarial train accuracy: 0.09375\n",
      "Current adversarial train loss: 3.8856778144836426\n",
      "\n",
      "Current batch: 150\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.101792097091675\n",
      "Current adversarial train accuracy: 0.1875\n",
      "Current adversarial train loss: 3.598473072052002\n",
      "\n",
      "Current batch: 160\n",
      "Current benign train accuracy: 0.671875\n",
      "Current benign train loss: 1.8267780542373657\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.964498519897461\n",
      "\n",
      "Current batch: 170\n",
      "Current benign train accuracy: 0.0625\n",
      "Current benign train loss: 3.479274272918701\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.773698568344116\n",
      "\n",
      "Current batch: 180\n",
      "Current benign train accuracy: 0.1015625\n",
      "Current benign train loss: 3.468657970428467\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.867121458053589\n",
      "\n",
      "Current batch: 190\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.6833863258361816\n",
      "Current adversarial train accuracy: 0.140625\n",
      "Current adversarial train loss: 3.872032642364502\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 1.9664579629898071\n",
      "Current adversarial train accuracy: 0.1484375\n",
      "Current adversarial train loss: 3.906259298324585\n",
      "\n",
      "Current batch: 210\n",
      "Current benign train accuracy: 0.1015625\n",
      "Current benign train loss: 3.486504077911377\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.9703402519226074\n",
      "\n",
      "Current batch: 220\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.977971076965332\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 3.842970848083496\n",
      "\n",
      "Current batch: 230\n",
      "Current benign train accuracy: 0.53125\n",
      "Current benign train loss: 2.803330659866333\n",
      "Current adversarial train accuracy: 0.0859375\n",
      "Current adversarial train loss: 4.152360916137695\n",
      "\n",
      "Current batch: 240\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.8900985717773438\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.8447258472442627\n",
      "\n",
      "Current batch: 250\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.438753843307495\n",
      "Current adversarial train accuracy: 0.140625\n",
      "Current adversarial train loss: 3.9098129272460938\n",
      "\n",
      "Current batch: 260\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 2.995582103729248\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.8160207271575928\n",
      "\n",
      "Current batch: 270\n",
      "Current benign train accuracy: 0.09375\n",
      "Current benign train loss: 3.5208358764648438\n",
      "Current adversarial train accuracy: 0.1640625\n",
      "Current adversarial train loss: 3.517953395843506\n",
      "\n",
      "Current batch: 280\n",
      "Current benign train accuracy: 0.09375\n",
      "Current benign train loss: 3.498145580291748\n",
      "Current adversarial train accuracy: 0.0546875\n",
      "Current adversarial train loss: 4.0341105461120605\n",
      "\n",
      "Current batch: 290\n",
      "Current benign train accuracy: 0.3828125\n",
      "Current benign train loss: 3.410308361053467\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 4.085932731628418\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.59375\n",
      "Current benign train loss: 2.9425301551818848\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 3.9458770751953125\n",
      "\n",
      "Current batch: 310\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.8855433464050293\n",
      "Current adversarial train accuracy: 0.1953125\n",
      "Current adversarial train loss: 3.7881593704223633\n",
      "\n",
      "Current batch: 320\n",
      "Current benign train accuracy: 0.3203125\n",
      "Current benign train loss: 3.462449789047241\n",
      "Current adversarial train accuracy: 0.1484375\n",
      "Current adversarial train loss: 3.991304874420166\n",
      "\n",
      "Current batch: 330\n",
      "Current benign train accuracy: 0.6015625\n",
      "Current benign train loss: 2.524855852127075\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.696774482727051\n",
      "\n",
      "Current batch: 340\n",
      "Current benign train accuracy: 0.5546875\n",
      "Current benign train loss: 2.9382705688476562\n",
      "Current adversarial train accuracy: 0.09375\n",
      "Current adversarial train loss: 3.979771137237549\n",
      "\n",
      "Current batch: 350\n",
      "Current benign train accuracy: 0.0703125\n",
      "Current benign train loss: 3.3465399742126465\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 4.090905666351318\n",
      "\n",
      "Current batch: 360\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 3.208491325378418\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 4.048081398010254\n",
      "\n",
      "Current batch: 370\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 3.317750930786133\n",
      "Current adversarial train accuracy: 0.1640625\n",
      "Current adversarial train loss: 3.6252670288085938\n",
      "\n",
      "Current batch: 380\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.401695489883423\n",
      "Current adversarial train accuracy: 0.171875\n",
      "Current adversarial train loss: 4.013949394226074\n",
      "\n",
      "Current batch: 390\n",
      "Current benign train accuracy: 0.2625\n",
      "Current benign train loss: 3.5312447547912598\n",
      "Current adversarial train accuracy: 0.125\n",
      "Current adversarial train loss: 4.15437126159668\n",
      "\n",
      "Total benign train accuarcy: tensor(42.7300, device='cuda:0')\n",
      "Total adversarial train accuarcy: tensor(13.7819, device='cuda:0')\n",
      "Total benign train loss: 1077.6598246097565\n",
      "Total adversarial train loss: 1534.7587311267853\n",
      "\n",
      "[ Test epoch: 1 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign test accuracy: 0.62\n",
      "Current benign test loss: 1.8093273639678955\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.770414113998413\n",
      "\n",
      "Current batch: 10\n",
      "Current benign test accuracy: 0.57\n",
      "Current benign test loss: 1.8811767101287842\n",
      "Current adversarial test accuracy: 0.12\n",
      "Current adversarial test loss: 3.9024219512939453\n",
      "\n",
      "Current batch: 20\n",
      "Current benign test accuracy: 0.59\n",
      "Current benign test loss: 1.8122373819351196\n",
      "Current adversarial test accuracy: 0.13\n",
      "Current adversarial test loss: 3.7644219398498535\n",
      "\n",
      "Current batch: 30\n",
      "Current benign test accuracy: 0.5\n",
      "Current benign test loss: 2.0118536949157715\n",
      "Current adversarial test accuracy: 0.14\n",
      "Current adversarial test loss: 3.9600656032562256\n",
      "\n",
      "Current batch: 40\n",
      "Current benign test accuracy: 0.57\n",
      "Current benign test loss: 1.8216787576675415\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.8129491806030273\n",
      "\n",
      "Current batch: 50\n",
      "Current benign test accuracy: 0.65\n",
      "Current benign test loss: 1.7738970518112183\n",
      "Current adversarial test accuracy: 0.2\n",
      "Current adversarial test loss: 3.6900293827056885\n",
      "\n",
      "Current batch: 60\n",
      "Current benign test accuracy: 0.55\n",
      "Current benign test loss: 1.9170196056365967\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.754526376724243\n",
      "\n",
      "Current batch: 70\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.8957414627075195\n",
      "Current adversarial test accuracy: 0.14\n",
      "Current adversarial test loss: 3.8857107162475586\n",
      "\n",
      "Current batch: 80\n",
      "Current benign test accuracy: 0.49\n",
      "Current benign test loss: 2.031493663787842\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.8735299110412598\n",
      "\n",
      "Current batch: 90\n",
      "Current benign test accuracy: 0.57\n",
      "Current benign test loss: 1.9643564224243164\n",
      "Current adversarial test accuracy: 0.19\n",
      "Current adversarial test loss: 3.8003604412078857\n",
      "\n",
      "Total benign test accuarcy: 57.76\n",
      "Total adversarial test Accuarcy: 16.28\n",
      "Total benign test loss: 185.58690762519836\n",
      "Total adversarial test loss: 378.66511583328247\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 2 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.2734375\n",
      "Current benign train loss: 3.528996229171753\n",
      "Current adversarial train accuracy: 0.1484375\n",
      "Current adversarial train loss: 4.035823345184326\n",
      "\n",
      "Current batch: 10\n",
      "Current benign train accuracy: 0.3125\n",
      "Current benign train loss: 3.472236394882202\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.766909599304199\n",
      "\n",
      "Current batch: 20\n",
      "Current benign train accuracy: 0.3359375\n",
      "Current benign train loss: 3.40116024017334\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.4865925312042236\n",
      "\n",
      "Current batch: 30\n",
      "Current benign train accuracy: 0.453125\n",
      "Current benign train loss: 3.211134433746338\n",
      "Current adversarial train accuracy: 0.1953125\n",
      "Current adversarial train loss: 3.7636051177978516\n",
      "\n",
      "Current batch: 40\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 3.0400707721710205\n",
      "Current adversarial train accuracy: 0.0703125\n",
      "Current adversarial train loss: 3.9645066261291504\n",
      "\n",
      "Current batch: 50\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 1.5708256959915161\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.7367300987243652\n",
      "\n",
      "Current batch: 60\n",
      "Current benign train accuracy: 0.6640625\n",
      "Current benign train loss: 1.9471843242645264\n",
      "Current adversarial train accuracy: 0.1328125\n",
      "Current adversarial train loss: 3.9493346214294434\n",
      "\n",
      "Current batch: 70\n",
      "Current benign train accuracy: 0.171875\n",
      "Current benign train loss: 3.51466965675354\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 4.070937633514404\n",
      "\n",
      "Current batch: 80\n",
      "Current benign train accuracy: 0.7734375\n",
      "Current benign train loss: 1.841295838356018\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.7717502117156982\n",
      "\n",
      "Current batch: 90\n",
      "Current benign train accuracy: 0.453125\n",
      "Current benign train loss: 3.2953779697418213\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.5455453395843506\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.125\n",
      "Current benign train loss: 3.5625691413879395\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.8619933128356934\n",
      "\n",
      "Current batch: 110\n",
      "Current benign train accuracy: 0.6796875\n",
      "Current benign train loss: 2.165165901184082\n",
      "Current adversarial train accuracy: 0.0625\n",
      "Current adversarial train loss: 4.148481845855713\n",
      "\n",
      "Current batch: 120\n",
      "Current benign train accuracy: 0.390625\n",
      "Current benign train loss: 3.3250088691711426\n",
      "Current adversarial train accuracy: 0.1328125\n",
      "Current adversarial train loss: 3.846768379211426\n",
      "\n",
      "Current batch: 130\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 3.224111557006836\n",
      "Current adversarial train accuracy: 0.1171875\n",
      "Current adversarial train loss: 3.940979480743408\n",
      "\n",
      "Current batch: 140\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.422347068786621\n",
      "Current adversarial train accuracy: 0.140625\n",
      "Current adversarial train loss: 3.980084180831909\n",
      "\n",
      "Current batch: 150\n",
      "Current benign train accuracy: 0.1875\n",
      "Current benign train loss: 3.5621094703674316\n",
      "Current adversarial train accuracy: 0.1953125\n",
      "Current adversarial train loss: 3.9980273246765137\n",
      "\n",
      "Current batch: 160\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.72084903717041\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.379894733428955\n",
      "\n",
      "Current batch: 170\n",
      "Current benign train accuracy: 0.640625\n",
      "Current benign train loss: 2.3911361694335938\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.9932451248168945\n",
      "\n",
      "Current batch: 180\n",
      "Current benign train accuracy: 0.515625\n",
      "Current benign train loss: 3.214808464050293\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.7168989181518555\n",
      "\n",
      "Current batch: 190\n",
      "Current benign train accuracy: 0.5546875\n",
      "Current benign train loss: 2.908043384552002\n",
      "Current adversarial train accuracy: 0.1640625\n",
      "Current adversarial train loss: 4.019248962402344\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.102419376373291\n",
      "Current adversarial train accuracy: 0.15625\n",
      "Current adversarial train loss: 3.9301083087921143\n",
      "\n",
      "Current batch: 210\n",
      "Current benign train accuracy: 0.375\n",
      "Current benign train loss: 3.2803759574890137\n",
      "Current adversarial train accuracy: 0.28125\n",
      "Current adversarial train loss: 3.7877261638641357\n",
      "\n",
      "Current batch: 220\n",
      "Current benign train accuracy: 0.3046875\n",
      "Current benign train loss: 3.4386277198791504\n",
      "Current adversarial train accuracy: 0.15625\n",
      "Current adversarial train loss: 3.812331199645996\n",
      "\n",
      "Current batch: 230\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 3.495699882507324\n",
      "Current adversarial train accuracy: 0.171875\n",
      "Current adversarial train loss: 4.08821964263916\n",
      "\n",
      "Current batch: 240\n",
      "Current benign train accuracy: 0.6640625\n",
      "Current benign train loss: 1.7945798635482788\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.9083375930786133\n",
      "\n",
      "Current batch: 250\n",
      "Current benign train accuracy: 0.65625\n",
      "Current benign train loss: 2.078673839569092\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 4.06795597076416\n",
      "\n",
      "Current batch: 260\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.7514266967773438\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.7019481658935547\n",
      "\n",
      "Current batch: 270\n",
      "Current benign train accuracy: 0.5\n",
      "Current benign train loss: 3.1904895305633545\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.572188377380371\n",
      "\n",
      "Current batch: 280\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.5489015579223633\n",
      "Current adversarial train accuracy: 0.2734375\n",
      "Current adversarial train loss: 3.443004608154297\n",
      "\n",
      "Current batch: 290\n",
      "Current benign train accuracy: 0.4921875\n",
      "Current benign train loss: 3.257380962371826\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.9637222290039062\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.3046875\n",
      "Current benign train loss: 3.585649013519287\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.788346767425537\n",
      "\n",
      "Current batch: 310\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.3398325443267822\n",
      "Current adversarial train accuracy: 0.0625\n",
      "Current adversarial train loss: 3.945991039276123\n",
      "\n",
      "Current batch: 320\n",
      "Current benign train accuracy: 0.15625\n",
      "Current benign train loss: 3.3736586570739746\n",
      "Current adversarial train accuracy: 0.140625\n",
      "Current adversarial train loss: 3.7566823959350586\n",
      "\n",
      "Current batch: 330\n",
      "Current benign train accuracy: 0.5078125\n",
      "Current benign train loss: 3.0939207077026367\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.8781661987304688\n",
      "\n",
      "Current batch: 340\n",
      "Current benign train accuracy: 0.671875\n",
      "Current benign train loss: 2.2927780151367188\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 4.166782379150391\n",
      "\n",
      "Current batch: 350\n",
      "Current benign train accuracy: 0.09375\n",
      "Current benign train loss: 3.36661434173584\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.853313446044922\n",
      "\n",
      "Current batch: 360\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.835298538208008\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.7560272216796875\n",
      "\n",
      "Current batch: 370\n",
      "Current benign train accuracy: 0.71875\n",
      "Current benign train loss: 1.582584023475647\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.4077887535095215\n",
      "\n",
      "Current batch: 380\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.2424612045288086\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 4.015420913696289\n",
      "\n",
      "Current batch: 390\n",
      "Current benign train accuracy: 0.625\n",
      "Current benign train loss: 2.0769784450531006\n",
      "Current adversarial train accuracy: 0.025\n",
      "Current adversarial train loss: 3.985182285308838\n",
      "\n",
      "Total benign train accuarcy: tensor(44.6476, device='cuda:0')\n",
      "Total adversarial train accuarcy: tensor(14.9513, device='cuda:0')\n",
      "Total benign train loss: 1050.2374633550644\n",
      "Total adversarial train loss: 1504.7973291873932\n",
      "\n",
      "[ Test epoch: 2 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign test accuracy: 0.65\n",
      "Current benign test loss: 1.6163941621780396\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.6560349464416504\n",
      "\n",
      "Current batch: 10\n",
      "Current benign test accuracy: 0.56\n",
      "Current benign test loss: 1.717323899269104\n",
      "Current adversarial test accuracy: 0.1\n",
      "Current adversarial test loss: 3.934816598892212\n",
      "\n",
      "Current batch: 20\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.7331727743148804\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.845688819885254\n",
      "\n",
      "Current batch: 30\n",
      "Current benign test accuracy: 0.59\n",
      "Current benign test loss: 1.8515217304229736\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.956453323364258\n",
      "\n",
      "Current batch: 40\n",
      "Current benign test accuracy: 0.65\n",
      "Current benign test loss: 1.7208179235458374\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.8845129013061523\n",
      "\n",
      "Current batch: 50\n",
      "Current benign test accuracy: 0.67\n",
      "Current benign test loss: 1.6014106273651123\n",
      "Current adversarial test accuracy: 0.23\n",
      "Current adversarial test loss: 3.7084391117095947\n",
      "\n",
      "Current batch: 60\n",
      "Current benign test accuracy: 0.58\n",
      "Current benign test loss: 1.8451521396636963\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.893630266189575\n",
      "\n",
      "Current batch: 70\n",
      "Current benign test accuracy: 0.59\n",
      "Current benign test loss: 1.7090163230895996\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.894913673400879\n",
      "\n",
      "Current batch: 80\n",
      "Current benign test accuracy: 0.53\n",
      "Current benign test loss: 1.8534791469573975\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.869271755218506\n",
      "\n",
      "Current batch: 90\n",
      "Current benign test accuracy: 0.52\n",
      "Current benign test loss: 1.913543701171875\n",
      "Current adversarial test accuracy: 0.14\n",
      "Current adversarial test loss: 4.025198936462402\n",
      "\n",
      "Total benign test accuarcy: 59.7\n",
      "Total adversarial test Accuarcy: 16.44\n",
      "Total benign test loss: 173.55902302265167\n",
      "Total adversarial test loss: 387.7925705909729\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 3 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.9598283767700195\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.9553914070129395\n",
      "\n",
      "Current batch: 10\n",
      "Current benign train accuracy: 0.34375\n",
      "Current benign train loss: 3.4420902729034424\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.6274266242980957\n",
      "\n",
      "Current batch: 20\n",
      "Current benign train accuracy: 0.71875\n",
      "Current benign train loss: 1.9191454648971558\n",
      "Current adversarial train accuracy: 0.109375\n",
      "Current adversarial train loss: 3.783426284790039\n",
      "\n",
      "Current batch: 30\n",
      "Current benign train accuracy: 0.1171875\n",
      "Current benign train loss: 3.4280312061309814\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.804482936859131\n",
      "\n",
      "Current batch: 40\n",
      "Current benign train accuracy: 0.390625\n",
      "Current benign train loss: 3.413479804992676\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.7340199947357178\n",
      "\n",
      "Current batch: 50\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.2533140182495117\n",
      "Current adversarial train accuracy: 0.125\n",
      "Current adversarial train loss: 3.971743106842041\n",
      "\n",
      "Current batch: 60\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.928278923034668\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.575244665145874\n",
      "\n",
      "Current batch: 70\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 1.6199244260787964\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 3.792128086090088\n",
      "\n",
      "Current batch: 80\n",
      "Current benign train accuracy: 0.421875\n",
      "Current benign train loss: 3.3388185501098633\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 4.015524864196777\n",
      "\n",
      "Current batch: 90\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.7081797122955322\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.852499008178711\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.944725513458252\n",
      "Current adversarial train accuracy: 0.1796875\n",
      "Current adversarial train loss: 4.027469635009766\n",
      "\n",
      "Current batch: 110\n",
      "Current benign train accuracy: 0.40625\n",
      "Current benign train loss: 3.2943410873413086\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 4.048591613769531\n",
      "\n",
      "Current batch: 120\n",
      "Current benign train accuracy: 0.484375\n",
      "Current benign train loss: 3.234663486480713\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.7622485160827637\n",
      "\n",
      "Current batch: 130\n",
      "Current benign train accuracy: 0.6796875\n",
      "Current benign train loss: 1.420687198638916\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.864522695541382\n",
      "\n",
      "Current batch: 140\n",
      "Current benign train accuracy: 0.3984375\n",
      "Current benign train loss: 3.3153324127197266\n",
      "Current adversarial train accuracy: 0.2421875\n",
      "Current adversarial train loss: 3.78647518157959\n",
      "\n",
      "Current batch: 150\n",
      "Current benign train accuracy: 0.609375\n",
      "Current benign train loss: 2.5469136238098145\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 4.041110992431641\n",
      "\n",
      "Current batch: 160\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.5048270225524902\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.5521323680877686\n",
      "\n",
      "Current batch: 170\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.7655624151229858\n",
      "Current adversarial train accuracy: 0.09375\n",
      "Current adversarial train loss: 3.9339170455932617\n",
      "\n",
      "Current batch: 180\n",
      "Current benign train accuracy: 0.625\n",
      "Current benign train loss: 1.5245554447174072\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 4.050679683685303\n",
      "\n",
      "Current batch: 190\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 3.210813283920288\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 4.146248817443848\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.512866973876953\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.891946792602539\n",
      "\n",
      "Current batch: 210\n",
      "Current benign train accuracy: 0.203125\n",
      "Current benign train loss: 3.513554096221924\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.7196524143218994\n",
      "\n",
      "Current batch: 220\n",
      "Current benign train accuracy: 0.328125\n",
      "Current benign train loss: 3.369868278503418\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 3.811579704284668\n",
      "\n",
      "Current batch: 230\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 1.6695544719696045\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.835645914077759\n",
      "\n",
      "Current batch: 240\n",
      "Current benign train accuracy: 0.6015625\n",
      "Current benign train loss: 2.792508125305176\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.5318284034729004\n",
      "\n",
      "Current batch: 250\n",
      "Current benign train accuracy: 0.6484375\n",
      "Current benign train loss: 2.147963285446167\n",
      "Current adversarial train accuracy: 0.15625\n",
      "Current adversarial train loss: 3.871987819671631\n",
      "\n",
      "Current batch: 260\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.8756332397460938\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.5873007774353027\n",
      "\n",
      "Current batch: 270\n",
      "Current benign train accuracy: 0.3515625\n",
      "Current benign train loss: 3.4670276641845703\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.4574203491210938\n",
      "\n",
      "Current batch: 280\n",
      "Current benign train accuracy: 0.5625\n",
      "Current benign train loss: 2.842071533203125\n",
      "Current adversarial train accuracy: 0.265625\n",
      "Current adversarial train loss: 3.8664309978485107\n",
      "\n",
      "Current batch: 290\n",
      "Current benign train accuracy: 0.6015625\n",
      "Current benign train loss: 2.7374703884124756\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 4.135119438171387\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.078125\n",
      "Current benign train loss: 3.2172935009002686\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.8287296295166016\n",
      "\n",
      "Current batch: 310\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.705727219581604\n",
      "Current adversarial train accuracy: 0.1484375\n",
      "Current adversarial train loss: 3.7352871894836426\n",
      "\n",
      "Current batch: 320\n",
      "Current benign train accuracy: 0.65625\n",
      "Current benign train loss: 2.060335874557495\n",
      "Current adversarial train accuracy: 0.1953125\n",
      "Current adversarial train loss: 3.4140453338623047\n",
      "\n",
      "Current batch: 330\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 2.025301456451416\n",
      "Current adversarial train accuracy: 0.0546875\n",
      "Current adversarial train loss: 4.002968788146973\n",
      "\n",
      "Current batch: 340\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.441648006439209\n",
      "Current adversarial train accuracy: 0.109375\n",
      "Current adversarial train loss: 3.912329912185669\n",
      "\n",
      "Current batch: 350\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 3.074583053588867\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.4454023838043213\n",
      "\n",
      "Current batch: 360\n",
      "Current benign train accuracy: 0.6796875\n",
      "Current benign train loss: 1.9924267530441284\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.946619987487793\n",
      "\n",
      "Current batch: 370\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.9200677871704102\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.6224353313446045\n",
      "\n",
      "Current batch: 380\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 3.312032461166382\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.8516616821289062\n",
      "\n",
      "Current batch: 390\n",
      "Current benign train accuracy: 0.0125\n",
      "Current benign train loss: 3.1790084838867188\n",
      "Current adversarial train accuracy: 0.0375\n",
      "Current adversarial train loss: 3.9914722442626953\n",
      "\n",
      "Total benign train accuarcy: tensor(47.2292, device='cuda:0')\n",
      "Total adversarial train accuarcy: tensor(15.5430, device='cuda:0')\n",
      "Total benign train loss: 1006.8812353610992\n",
      "Total adversarial train loss: 1491.1694326400757\n",
      "\n",
      "[ Test epoch: 3 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign test accuracy: 0.65\n",
      "Current benign test loss: 1.7125719785690308\n",
      "Current adversarial test accuracy: 0.19\n",
      "Current adversarial test loss: 3.664010524749756\n",
      "\n",
      "Current batch: 10\n",
      "Current benign test accuracy: 0.53\n",
      "Current benign test loss: 1.8114843368530273\n",
      "Current adversarial test accuracy: 0.15\n",
      "Current adversarial test loss: 3.8770089149475098\n",
      "\n",
      "Current batch: 20\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.7434555292129517\n",
      "Current adversarial test accuracy: 0.15\n",
      "Current adversarial test loss: 3.7263495922088623\n",
      "\n",
      "Current batch: 30\n",
      "Current benign test accuracy: 0.54\n",
      "Current benign test loss: 1.9520764350891113\n",
      "Current adversarial test accuracy: 0.11\n",
      "Current adversarial test loss: 3.853086471557617\n",
      "\n",
      "Current batch: 40\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.711500644683838\n",
      "Current adversarial test accuracy: 0.21\n",
      "Current adversarial test loss: 3.630877733230591\n",
      "\n",
      "Current batch: 50\n",
      "Current benign test accuracy: 0.59\n",
      "Current benign test loss: 1.7012372016906738\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.607239007949829\n",
      "\n",
      "Current batch: 60\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.8572598695755005\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.7652664184570312\n",
      "\n",
      "Current batch: 70\n",
      "Current benign test accuracy: 0.52\n",
      "Current benign test loss: 1.8996294736862183\n",
      "Current adversarial test accuracy: 0.13\n",
      "Current adversarial test loss: 3.8896100521087646\n",
      "\n",
      "Current batch: 80\n",
      "Current benign test accuracy: 0.58\n",
      "Current benign test loss: 1.9195176362991333\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.853907585144043\n",
      "\n",
      "Current batch: 90\n",
      "Current benign test accuracy: 0.52\n",
      "Current benign test loss: 1.9772073030471802\n",
      "Current adversarial test accuracy: 0.2\n",
      "Current adversarial test loss: 3.8545005321502686\n",
      "\n",
      "Total benign test accuarcy: 57.75\n",
      "Total adversarial test Accuarcy: 17.04\n",
      "Total benign test loss: 183.0260568857193\n",
      "Total adversarial test loss: 377.50450706481934\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 4 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.740867853164673\n",
      "Current adversarial train accuracy: 0.25\n",
      "Current adversarial train loss: 3.6884560585021973\n",
      "\n",
      "Current batch: 10\n",
      "Current benign train accuracy: 0.328125\n",
      "Current benign train loss: 3.3662476539611816\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.756305694580078\n",
      "\n",
      "Current batch: 20\n",
      "Current benign train accuracy: 0.4296875\n",
      "Current benign train loss: 3.315009117126465\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.7247657775878906\n",
      "\n",
      "Current batch: 30\n",
      "Current benign train accuracy: 0.6171875\n",
      "Current benign train loss: 2.7363440990448\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.7864837646484375\n",
      "\n",
      "Current batch: 40\n",
      "Current benign train accuracy: 0.5703125\n",
      "Current benign train loss: 3.001826763153076\n",
      "Current adversarial train accuracy: 0.046875\n",
      "Current adversarial train loss: 3.946324586868286\n",
      "\n",
      "Current batch: 50\n",
      "Current benign train accuracy: 0.3515625\n",
      "Current benign train loss: 3.320685863494873\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.595126152038574\n",
      "\n",
      "Current batch: 60\n",
      "Current benign train accuracy: 0.34375\n",
      "Current benign train loss: 3.371171236038208\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.2872841358184814\n",
      "\n",
      "Current batch: 70\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 1.371062159538269\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.3706767559051514\n",
      "\n",
      "Current batch: 80\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.405216693878174\n",
      "Current adversarial train accuracy: 0.2421875\n",
      "Current adversarial train loss: 3.5089211463928223\n",
      "\n",
      "Current batch: 90\n",
      "Current benign train accuracy: 0.3203125\n",
      "Current benign train loss: 3.529139995574951\n",
      "Current adversarial train accuracy: 0.09375\n",
      "Current adversarial train loss: 4.017549514770508\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 3.2525904178619385\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.4092793464660645\n",
      "\n",
      "Current batch: 110\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 3.252695083618164\n",
      "Current adversarial train accuracy: 0.1796875\n",
      "Current adversarial train loss: 3.618626594543457\n",
      "\n",
      "Current batch: 120\n",
      "Current benign train accuracy: 0.3828125\n",
      "Current benign train loss: 3.298964023590088\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.797450065612793\n",
      "\n",
      "Current batch: 130\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.6275370121002197\n",
      "Current adversarial train accuracy: 0.1953125\n",
      "Current adversarial train loss: 3.360600233078003\n",
      "\n",
      "Current batch: 140\n",
      "Current benign train accuracy: 0.0859375\n",
      "Current benign train loss: 3.3854219913482666\n",
      "Current adversarial train accuracy: 0.1875\n",
      "Current adversarial train loss: 4.0002665519714355\n",
      "\n",
      "Current batch: 150\n",
      "Current benign train accuracy: 0.1640625\n",
      "Current benign train loss: 3.4969520568847656\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.9714698791503906\n",
      "\n",
      "Current batch: 160\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 2.017651319503784\n",
      "Current adversarial train accuracy: 0.1796875\n",
      "Current adversarial train loss: 3.9054300785064697\n",
      "\n",
      "Current batch: 170\n",
      "Current benign train accuracy: 0.65625\n",
      "Current benign train loss: 2.146223783493042\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.8218815326690674\n",
      "\n",
      "Current batch: 180\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.875387191772461\n",
      "Current adversarial train accuracy: 0.0546875\n",
      "Current adversarial train loss: 3.89695405960083\n",
      "\n",
      "Current batch: 190\n",
      "Current benign train accuracy: 0.71875\n",
      "Current benign train loss: 2.334367036819458\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.7094850540161133\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.6796875\n",
      "Current benign train loss: 2.639220714569092\n",
      "Current adversarial train accuracy: 0.171875\n",
      "Current adversarial train loss: 3.9550554752349854\n",
      "\n",
      "Current batch: 210\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 1.8590199947357178\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.572880744934082\n",
      "\n",
      "Current batch: 220\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 1.798709750175476\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 3.6259756088256836\n",
      "\n",
      "Current batch: 230\n",
      "Current benign train accuracy: 0.6484375\n",
      "Current benign train loss: 2.7486989498138428\n",
      "Current adversarial train accuracy: 0.1640625\n",
      "Current adversarial train loss: 3.730335235595703\n",
      "\n",
      "Current batch: 240\n",
      "Current benign train accuracy: 0.53125\n",
      "Current benign train loss: 3.1468098163604736\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.6419601440429688\n",
      "\n",
      "Current batch: 250\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 2.275090456008911\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 3.909777879714966\n",
      "\n",
      "Current batch: 260\n",
      "Current benign train accuracy: 0.3125\n",
      "Current benign train loss: 3.4172325134277344\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.8721280097961426\n",
      "\n",
      "Current batch: 270\n",
      "Current benign train accuracy: 0.2578125\n",
      "Current benign train loss: 3.5546884536743164\n",
      "Current adversarial train accuracy: 0.1640625\n",
      "Current adversarial train loss: 3.6695737838745117\n",
      "\n",
      "Current batch: 280\n",
      "Current benign train accuracy: 0.0625\n",
      "Current benign train loss: 3.4390926361083984\n",
      "Current adversarial train accuracy: 0.1484375\n",
      "Current adversarial train loss: 4.029428482055664\n",
      "\n",
      "Current batch: 290\n",
      "Current benign train accuracy: 0.125\n",
      "Current benign train loss: 3.365131378173828\n",
      "Current adversarial train accuracy: 0.09375\n",
      "Current adversarial train loss: 4.049056053161621\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6796875\n",
      "Current benign train loss: 1.8252620697021484\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 3.8652122020721436\n",
      "\n",
      "Current batch: 310\n",
      "Current benign train accuracy: 0.546875\n",
      "Current benign train loss: 3.1470837593078613\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.2799081802368164\n",
      "\n",
      "Current batch: 320\n",
      "Current benign train accuracy: 0.6796875\n",
      "Current benign train loss: 1.6124845743179321\n",
      "Current adversarial train accuracy: 0.046875\n",
      "Current adversarial train loss: 4.011218070983887\n",
      "\n",
      "Current batch: 330\n",
      "Current benign train accuracy: 0.6796875\n",
      "Current benign train loss: 1.6990859508514404\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 3.982633113861084\n",
      "\n",
      "Current batch: 340\n",
      "Current benign train accuracy: 0.640625\n",
      "Current benign train loss: 2.4456002712249756\n",
      "Current adversarial train accuracy: 0.2421875\n",
      "Current adversarial train loss: 3.22682523727417\n",
      "\n",
      "Current batch: 350\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 2.9354147911071777\n",
      "Current adversarial train accuracy: 0.1875\n",
      "Current adversarial train loss: 3.490669012069702\n",
      "\n",
      "Current batch: 360\n",
      "Current benign train accuracy: 0.1484375\n",
      "Current benign train loss: 3.4911746978759766\n",
      "Current adversarial train accuracy: 0.0859375\n",
      "Current adversarial train loss: 3.9249000549316406\n",
      "\n",
      "Current batch: 370\n",
      "Current benign train accuracy: 0.1875\n",
      "Current benign train loss: 3.436455249786377\n",
      "Current adversarial train accuracy: 0.1015625\n",
      "Current adversarial train loss: 4.060574531555176\n",
      "\n",
      "Current batch: 380\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 2.1696770191192627\n",
      "Current adversarial train accuracy: 0.1875\n",
      "Current adversarial train loss: 3.7612247467041016\n",
      "\n",
      "Current batch: 390\n",
      "Current benign train accuracy: 0.4625\n",
      "Current benign train loss: 3.172020673751831\n",
      "Current adversarial train accuracy: 0.025\n",
      "Current adversarial train loss: 3.820359706878662\n",
      "\n",
      "Total benign train accuarcy: tensor(44.4637, device='cuda:0')\n",
      "Total adversarial train accuarcy: tensor(16.2153, device='cuda:0')\n",
      "Total benign train loss: 1051.3685805797577\n",
      "Total adversarial train loss: 1469.5512375831604\n",
      "\n",
      "[ Test epoch: 4 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.6638002395629883\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.8782293796539307\n",
      "\n",
      "Current batch: 10\n",
      "Current benign test accuracy: 0.56\n",
      "Current benign test loss: 1.6321531534194946\n",
      "Current adversarial test accuracy: 0.1\n",
      "Current adversarial test loss: 3.940277099609375\n",
      "\n",
      "Current batch: 20\n",
      "Current benign test accuracy: 0.54\n",
      "Current benign test loss: 1.7256903648376465\n",
      "Current adversarial test accuracy: 0.16\n",
      "Current adversarial test loss: 3.961338996887207\n",
      "\n",
      "Current batch: 30\n",
      "Current benign test accuracy: 0.57\n",
      "Current benign test loss: 1.7671897411346436\n",
      "Current adversarial test accuracy: 0.14\n",
      "Current adversarial test loss: 3.989417314529419\n",
      "\n",
      "Current batch: 40\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.6467055082321167\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.8693549633026123\n",
      "\n",
      "Current batch: 50\n",
      "Current benign test accuracy: 0.62\n",
      "Current benign test loss: 1.6542750597000122\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.781562089920044\n",
      "\n",
      "Current batch: 60\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.640965223312378\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.786764621734619\n",
      "\n",
      "Current batch: 70\n",
      "Current benign test accuracy: 0.55\n",
      "Current benign test loss: 1.705736517906189\n",
      "Current adversarial test accuracy: 0.09\n",
      "Current adversarial test loss: 3.9948182106018066\n",
      "\n",
      "Current batch: 80\n",
      "Current benign test accuracy: 0.57\n",
      "Current benign test loss: 1.7320233583450317\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.819423198699951\n",
      "\n",
      "Current batch: 90\n",
      "Current benign test accuracy: 0.51\n",
      "Current benign test loss: 1.8603386878967285\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.995903730392456\n",
      "\n",
      "Total benign test accuarcy: 59.7\n",
      "Total adversarial test Accuarcy: 15.98\n",
      "Total benign test loss: 166.92952644824982\n",
      "Total adversarial test loss: 389.19848895072937\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 5 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.671875\n",
      "Current benign train loss: 2.713956356048584\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.6351895332336426\n",
      "\n",
      "Current batch: 10\n",
      "Current benign train accuracy: 0.265625\n",
      "Current benign train loss: 3.3038387298583984\n",
      "Current adversarial train accuracy: 0.265625\n",
      "Current adversarial train loss: 3.7258827686309814\n",
      "\n",
      "Current batch: 20\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 1.8954523801803589\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.6619768142700195\n",
      "\n",
      "Current batch: 30\n",
      "Current benign train accuracy: 0.6640625\n",
      "Current benign train loss: 2.393348455429077\n",
      "Current adversarial train accuracy: 0.1640625\n",
      "Current adversarial train loss: 3.9888761043548584\n",
      "\n",
      "Current batch: 40\n",
      "Current benign train accuracy: 0.5078125\n",
      "Current benign train loss: 3.1235270500183105\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.777303457260132\n",
      "\n",
      "Current batch: 50\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.6096010208129883\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.8196117877960205\n",
      "\n",
      "Current batch: 60\n",
      "Current benign train accuracy: 0.0703125\n",
      "Current benign train loss: 3.120060920715332\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.7534689903259277\n",
      "\n",
      "Current batch: 70\n",
      "Current benign train accuracy: 0.671875\n",
      "Current benign train loss: 2.248499631881714\n",
      "Current adversarial train accuracy: 0.046875\n",
      "Current adversarial train loss: 3.6455140113830566\n",
      "\n",
      "Current batch: 80\n",
      "Current benign train accuracy: 0.6015625\n",
      "Current benign train loss: 2.7528719902038574\n",
      "Current adversarial train accuracy: 0.25\n",
      "Current adversarial train loss: 3.6111435890197754\n",
      "\n",
      "Current batch: 90\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 2.9976954460144043\n",
      "Current adversarial train accuracy: 0.1796875\n",
      "Current adversarial train loss: 3.967456579208374\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.5859375\n",
      "Current benign train loss: 2.815171003341675\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 3.398874282836914\n",
      "\n",
      "Current batch: 110\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 1.9462738037109375\n",
      "Current adversarial train accuracy: 0.1875\n",
      "Current adversarial train loss: 3.888033390045166\n",
      "\n",
      "Current batch: 120\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 2.060065746307373\n",
      "Current adversarial train accuracy: 0.2421875\n",
      "Current adversarial train loss: 3.371281623840332\n",
      "\n",
      "Current batch: 130\n",
      "Current benign train accuracy: 0.640625\n",
      "Current benign train loss: 2.432607650756836\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.227099895477295\n",
      "\n",
      "Current batch: 140\n",
      "Current benign train accuracy: 0.359375\n",
      "Current benign train loss: 3.465963363647461\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.8101487159729004\n",
      "\n",
      "Current batch: 150\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 2.3160414695739746\n",
      "Current adversarial train accuracy: 0.140625\n",
      "Current adversarial train loss: 3.7993502616882324\n",
      "\n",
      "Current batch: 160\n",
      "Current benign train accuracy: 0.703125\n",
      "Current benign train loss: 1.9410874843597412\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.667811155319214\n",
      "\n",
      "Current batch: 170\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.7516937255859375\n",
      "Current adversarial train accuracy: 0.2734375\n",
      "Current adversarial train loss: 3.3027381896972656\n",
      "\n",
      "Current batch: 180\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 1.9780625104904175\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.38838267326355\n",
      "\n",
      "Current batch: 190\n",
      "Current benign train accuracy: 0.734375\n",
      "Current benign train loss: 1.5439257621765137\n",
      "Current adversarial train accuracy: 0.2421875\n",
      "Current adversarial train loss: 3.52708101272583\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 2.800588607788086\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.154442548751831\n",
      "\n",
      "Current batch: 210\n",
      "Current benign train accuracy: 0.0546875\n",
      "Current benign train loss: 3.030916690826416\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.288591146469116\n",
      "\n",
      "Current batch: 220\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 2.8323445320129395\n",
      "Current adversarial train accuracy: 0.359375\n",
      "Current adversarial train loss: 3.1626620292663574\n",
      "\n",
      "Current batch: 230\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.7662487030029297\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.122516393661499\n",
      "\n",
      "Current batch: 240\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.3402280807495117\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.07596755027771\n",
      "\n",
      "Current batch: 250\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.5201940536499023\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.7359399795532227\n",
      "\n",
      "Current batch: 260\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.958197832107544\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.223273277282715\n",
      "\n",
      "Current batch: 270\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.6340042352676392\n",
      "Current adversarial train accuracy: 0.0546875\n",
      "Current adversarial train loss: 3.6745047569274902\n",
      "\n",
      "Current batch: 280\n",
      "Current benign train accuracy: 0.25\n",
      "Current benign train loss: 3.424976348876953\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.836261749267578\n",
      "\n",
      "Current batch: 290\n",
      "Current benign train accuracy: 0.75\n",
      "Current benign train loss: 1.4368658065795898\n",
      "Current adversarial train accuracy: 0.15625\n",
      "Current adversarial train loss: 3.879241466522217\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.0546875\n",
      "Current benign train loss: 1.4995484352111816\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.858022689819336\n",
      "\n",
      "Current batch: 310\n",
      "Current benign train accuracy: 0.6171875\n",
      "Current benign train loss: 2.7871952056884766\n",
      "Current adversarial train accuracy: 0.046875\n",
      "Current adversarial train loss: 3.798053741455078\n",
      "\n",
      "Current batch: 320\n",
      "Current benign train accuracy: 0.546875\n",
      "Current benign train loss: 2.967125415802002\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.685676336288452\n",
      "\n",
      "Current batch: 330\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 1.4052979946136475\n",
      "Current adversarial train accuracy: 0.0546875\n",
      "Current adversarial train loss: 3.843256950378418\n",
      "\n",
      "Current batch: 340\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.9096708297729492\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.7759909629821777\n",
      "\n",
      "Current batch: 350\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.601867198944092\n",
      "Current adversarial train accuracy: 0.265625\n",
      "Current adversarial train loss: 3.229121685028076\n",
      "\n",
      "Current batch: 360\n",
      "Current benign train accuracy: 0.0625\n",
      "Current benign train loss: 2.7412796020507812\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.4627413749694824\n",
      "\n",
      "Current batch: 370\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 1.7271273136138916\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.66943359375\n",
      "\n",
      "Current batch: 380\n",
      "Current benign train accuracy: 0.7265625\n",
      "Current benign train loss: 1.6373586654663086\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.798403263092041\n",
      "\n",
      "Current batch: 390\n",
      "Current benign train accuracy: 0.6875\n",
      "Current benign train loss: 1.5108933448791504\n",
      "Current adversarial train accuracy: 0.0375\n",
      "Current adversarial train loss: 4.066067695617676\n",
      "\n",
      "Total benign train accuarcy: tensor(49.6024, device='cuda:0')\n",
      "Total adversarial train accuarcy: tensor(18.7655, device='cuda:0')\n",
      "Total benign train loss: 981.804593205452\n",
      "Total adversarial train loss: 1414.6016511917114\n",
      "\n",
      "[ Test epoch: 5 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign test accuracy: 0.67\n",
      "Current benign test loss: 1.559770107269287\n",
      "Current adversarial test accuracy: 0.23\n",
      "Current adversarial test loss: 3.6711156368255615\n",
      "\n",
      "Current batch: 10\n",
      "Current benign test accuracy: 0.63\n",
      "Current benign test loss: 1.5750558376312256\n",
      "Current adversarial test accuracy: 0.16\n",
      "Current adversarial test loss: 3.8207314014434814\n",
      "\n",
      "Current batch: 20\n",
      "Current benign test accuracy: 0.56\n",
      "Current benign test loss: 1.6185417175292969\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.7276041507720947\n",
      "\n",
      "Current batch: 30\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.7380784749984741\n",
      "Current adversarial test accuracy: 0.15\n",
      "Current adversarial test loss: 3.86061429977417\n",
      "\n",
      "Current batch: 40\n",
      "Current benign test accuracy: 0.63\n",
      "Current benign test loss: 1.5887271165847778\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.724327802658081\n",
      "\n",
      "Current batch: 50\n",
      "Current benign test accuracy: 0.63\n",
      "Current benign test loss: 1.569474458694458\n",
      "Current adversarial test accuracy: 0.21\n",
      "Current adversarial test loss: 3.5896482467651367\n",
      "\n",
      "Current batch: 60\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.6514098644256592\n",
      "Current adversarial test accuracy: 0.21\n",
      "Current adversarial test loss: 3.6839468479156494\n",
      "\n",
      "Current batch: 70\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.6224287748336792\n",
      "Current adversarial test accuracy: 0.15\n",
      "Current adversarial test loss: 3.7804131507873535\n",
      "\n",
      "Current batch: 80\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.6734882593154907\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.7036333084106445\n",
      "\n",
      "Current batch: 90\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.7770684957504272\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.8773064613342285\n",
      "\n",
      "Total benign test accuarcy: 62.96\n",
      "Total adversarial test Accuarcy: 17.82\n",
      "Total benign test loss: 160.91098976135254\n",
      "Total adversarial test loss: 374.5555901527405\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 6 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.4513630867004395\n",
      "Current adversarial train accuracy: 0.25\n",
      "Current adversarial train loss: 3.6621627807617188\n",
      "\n",
      "Current batch: 10\n",
      "Current benign train accuracy: 0.6875\n",
      "Current benign train loss: 2.3281798362731934\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.6254630088806152\n",
      "\n",
      "Current batch: 20\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.353052854537964\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.4926400184631348\n",
      "\n",
      "Current batch: 30\n",
      "Current benign train accuracy: 0.53125\n",
      "Current benign train loss: 3.205673933029175\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.1548688411712646\n",
      "\n",
      "Current batch: 40\n",
      "Current benign train accuracy: 0.6328125\n",
      "Current benign train loss: 2.790555477142334\n",
      "Current adversarial train accuracy: 0.0703125\n",
      "Current adversarial train loss: 3.852956771850586\n",
      "\n",
      "Current batch: 50\n",
      "Current benign train accuracy: 0.7265625\n",
      "Current benign train loss: 1.2166935205459595\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.460533857345581\n",
      "\n",
      "Current batch: 60\n",
      "Current benign train accuracy: 0.78125\n",
      "Current benign train loss: 1.8973462581634521\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.564788818359375\n",
      "\n",
      "Current batch: 70\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.4510186910629272\n",
      "Current adversarial train accuracy: 0.1953125\n",
      "Current adversarial train loss: 3.7605295181274414\n",
      "\n",
      "Current batch: 80\n",
      "Current benign train accuracy: 0.3359375\n",
      "Current benign train loss: 3.228257417678833\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.218334674835205\n",
      "\n",
      "Current batch: 90\n",
      "Current benign train accuracy: 0.0859375\n",
      "Current benign train loss: 3.0993053913116455\n",
      "Current adversarial train accuracy: 0.046875\n",
      "Current adversarial train loss: 3.820439338684082\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 3.2379536628723145\n",
      "Current adversarial train accuracy: 0.1875\n",
      "Current adversarial train loss: 4.0382490158081055\n",
      "\n",
      "Current batch: 110\n",
      "Current benign train accuracy: 0.609375\n",
      "Current benign train loss: 2.813915729522705\n",
      "Current adversarial train accuracy: 0.0546875\n",
      "Current adversarial train loss: 3.7258872985839844\n",
      "\n",
      "Current batch: 120\n",
      "Current benign train accuracy: 0.328125\n",
      "Current benign train loss: 3.2756781578063965\n",
      "Current adversarial train accuracy: 0.3203125\n",
      "Current adversarial train loss: 3.6058740615844727\n",
      "\n",
      "Current batch: 130\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.2221696376800537\n",
      "Current adversarial train accuracy: 0.2421875\n",
      "Current adversarial train loss: 3.3346052169799805\n",
      "\n",
      "Current batch: 140\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 1.5859456062316895\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.4239377975463867\n",
      "\n",
      "Current batch: 150\n",
      "Current benign train accuracy: 0.34375\n",
      "Current benign train loss: 3.4104552268981934\n",
      "Current adversarial train accuracy: 0.0625\n",
      "Current adversarial train loss: 3.722027540206909\n",
      "\n",
      "Current batch: 160\n",
      "Current benign train accuracy: 0.5390625\n",
      "Current benign train loss: 3.0398073196411133\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.554410934448242\n",
      "\n",
      "Current batch: 170\n",
      "Current benign train accuracy: 0.453125\n",
      "Current benign train loss: 3.2365212440490723\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.461118698120117\n",
      "\n",
      "Current batch: 180\n",
      "Current benign train accuracy: 0.4296875\n",
      "Current benign train loss: 3.2716126441955566\n",
      "Current adversarial train accuracy: 0.1796875\n",
      "Current adversarial train loss: 3.8526554107666016\n",
      "\n",
      "Current batch: 190\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 1.3310143947601318\n",
      "Current adversarial train accuracy: 0.2421875\n",
      "Current adversarial train loss: 3.7524099349975586\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.834573268890381\n",
      "Current adversarial train accuracy: 0.2734375\n",
      "Current adversarial train loss: 3.648932933807373\n",
      "\n",
      "Current batch: 210\n",
      "Current benign train accuracy: 0.546875\n",
      "Current benign train loss: 2.9648289680480957\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.514346122741699\n",
      "\n",
      "Current batch: 220\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.957617163658142\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.640383005142212\n",
      "\n",
      "Current batch: 230\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 2.043168544769287\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.241666316986084\n",
      "\n",
      "Current batch: 240\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.593230724334717\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.5747177600860596\n",
      "\n",
      "Current batch: 250\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 2.6654443740844727\n",
      "Current adversarial train accuracy: 0.0859375\n",
      "Current adversarial train loss: 3.8330917358398438\n",
      "\n",
      "Current batch: 260\n",
      "Current benign train accuracy: 0.7734375\n",
      "Current benign train loss: 2.0856571197509766\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 2.84975004196167\n",
      "\n",
      "Current batch: 270\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.3451378345489502\n",
      "Current adversarial train accuracy: 0.25\n",
      "Current adversarial train loss: 3.2061219215393066\n",
      "\n",
      "Current batch: 280\n",
      "Current benign train accuracy: 0.5234375\n",
      "Current benign train loss: 3.0622639656066895\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.775362253189087\n",
      "\n",
      "Current batch: 290\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 1.3775653839111328\n",
      "Current adversarial train accuracy: 0.3203125\n",
      "Current adversarial train loss: 3.6213841438293457\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.40625\n",
      "Current benign train loss: 3.4128994941711426\n",
      "Current adversarial train accuracy: 0.1875\n",
      "Current adversarial train loss: 3.7748706340789795\n",
      "\n",
      "Current batch: 310\n",
      "Current benign train accuracy: 0.734375\n",
      "Current benign train loss: 1.5749280452728271\n",
      "Current adversarial train accuracy: 0.046875\n",
      "Current adversarial train loss: 3.800096035003662\n",
      "\n",
      "Current batch: 320\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 1.645371675491333\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.264366626739502\n",
      "\n",
      "Current batch: 330\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.2507882118225098\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.236788034439087\n",
      "\n",
      "Current batch: 340\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 1.795000672340393\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.3212733268737793\n",
      "\n",
      "Current batch: 350\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 3.0108442306518555\n",
      "Current adversarial train accuracy: 0.171875\n",
      "Current adversarial train loss: 3.807539939880371\n",
      "\n",
      "Current batch: 360\n",
      "Current benign train accuracy: 0.5234375\n",
      "Current benign train loss: 3.1474227905273438\n",
      "Current adversarial train accuracy: 0.1328125\n",
      "Current adversarial train loss: 3.7315075397491455\n",
      "\n",
      "Current batch: 370\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 1.414892554283142\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.6721041202545166\n",
      "\n",
      "Current batch: 380\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.7534767389297485\n",
      "Current adversarial train accuracy: 0.265625\n",
      "Current adversarial train loss: 3.303469657897949\n",
      "\n",
      "Current batch: 390\n",
      "Current benign train accuracy: 0.75\n",
      "Current benign train loss: 1.575705647468567\n",
      "Current adversarial train accuracy: 0.1\n",
      "Current adversarial train loss: 3.794060468673706\n",
      "\n",
      "Total benign train accuarcy: tensor(51.5330, device='cuda:0')\n",
      "Total adversarial train accuarcy: tensor(19.8327, device='cuda:0')\n",
      "Total benign train loss: 956.2553548812866\n",
      "Total adversarial train loss: 1394.2453355789185\n",
      "\n",
      "[ Test epoch: 6 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign test accuracy: 0.69\n",
      "Current benign test loss: 1.4920703172683716\n",
      "Current adversarial test accuracy: 0.26\n",
      "Current adversarial test loss: 3.6143720149993896\n",
      "\n",
      "Current batch: 10\n",
      "Current benign test accuracy: 0.65\n",
      "Current benign test loss: 1.5046164989471436\n",
      "Current adversarial test accuracy: 0.16\n",
      "Current adversarial test loss: 3.797670602798462\n",
      "\n",
      "Current batch: 20\n",
      "Current benign test accuracy: 0.62\n",
      "Current benign test loss: 1.5088398456573486\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.639122724533081\n",
      "\n",
      "Current batch: 30\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.6792404651641846\n",
      "Current adversarial test accuracy: 0.16\n",
      "Current adversarial test loss: 3.824091911315918\n",
      "\n",
      "Current batch: 40\n",
      "Current benign test accuracy: 0.67\n",
      "Current benign test loss: 1.497948169708252\n",
      "Current adversarial test accuracy: 0.19\n",
      "Current adversarial test loss: 3.677586555480957\n",
      "\n",
      "Current batch: 50\n",
      "Current benign test accuracy: 0.67\n",
      "Current benign test loss: 1.4941314458847046\n",
      "Current adversarial test accuracy: 0.2\n",
      "Current adversarial test loss: 3.539128065109253\n",
      "\n",
      "Current batch: 60\n",
      "Current benign test accuracy: 0.64\n",
      "Current benign test loss: 1.565278172492981\n",
      "Current adversarial test accuracy: 0.23\n",
      "Current adversarial test loss: 3.600059747695923\n",
      "\n",
      "Current batch: 70\n",
      "Current benign test accuracy: 0.62\n",
      "Current benign test loss: 1.5368536710739136\n",
      "Current adversarial test accuracy: 0.14\n",
      "Current adversarial test loss: 3.750728130340576\n",
      "\n",
      "Current batch: 80\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.6065878868103027\n",
      "Current adversarial test accuracy: 0.21\n",
      "Current adversarial test loss: 3.700199604034424\n",
      "\n",
      "Current batch: 90\n",
      "Current benign test accuracy: 0.63\n",
      "Current benign test loss: 1.6923199892044067\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.8196828365325928\n",
      "\n",
      "Total benign test accuarcy: 64.53\n",
      "Total adversarial test Accuarcy: 18.6\n",
      "Total benign test loss: 152.8187474012375\n",
      "Total adversarial test loss: 369.5308656692505\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 7 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.2127474546432495\n",
      "Current adversarial train accuracy: 0.2890625\n",
      "Current adversarial train loss: 3.35628342628479\n",
      "\n",
      "Current batch: 10\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.605041742324829\n",
      "Current adversarial train accuracy: 0.1875\n",
      "Current adversarial train loss: 3.8040428161621094\n",
      "\n",
      "Current batch: 20\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 1.9597885608673096\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.7286438941955566\n",
      "\n",
      "Current batch: 30\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 1.2699207067489624\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.816099166870117\n",
      "\n",
      "Current batch: 40\n",
      "Current benign train accuracy: 0.75\n",
      "Current benign train loss: 1.4052705764770508\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.6444380283355713\n",
      "\n",
      "Current batch: 50\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.3097152709960938\n",
      "Current adversarial train accuracy: 0.25\n",
      "Current adversarial train loss: 3.1958017349243164\n",
      "\n",
      "Current batch: 60\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 3.0775833129882812\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.686087131500244\n",
      "\n",
      "Current batch: 70\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 2.592010021209717\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.854299545288086\n",
      "\n",
      "Current batch: 80\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 1.2953872680664062\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.2302675247192383\n",
      "\n",
      "Current batch: 90\n",
      "Current benign train accuracy: 0.1328125\n",
      "Current benign train loss: 3.211238384246826\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.7482738494873047\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.796875\n",
      "Current benign train loss: 1.6076961755752563\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.2374422550201416\n",
      "\n",
      "Current batch: 110\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 1.6781165599822998\n",
      "Current adversarial train accuracy: 0.3046875\n",
      "Current adversarial train loss: 3.5587170124053955\n",
      "\n",
      "Current batch: 120\n",
      "Current benign train accuracy: 0.515625\n",
      "Current benign train loss: 3.0515763759613037\n",
      "Current adversarial train accuracy: 0.0625\n",
      "Current adversarial train loss: 3.7976791858673096\n",
      "\n",
      "Current batch: 130\n",
      "Current benign train accuracy: 0.578125\n",
      "Current benign train loss: 2.857664108276367\n",
      "Current adversarial train accuracy: 0.1484375\n",
      "Current adversarial train loss: 3.999220848083496\n",
      "\n",
      "Current batch: 140\n",
      "Current benign train accuracy: 0.6015625\n",
      "Current benign train loss: 2.9464035034179688\n",
      "Current adversarial train accuracy: 0.3046875\n",
      "Current adversarial train loss: 3.272329330444336\n",
      "\n",
      "Current batch: 150\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 2.701082468032837\n",
      "Current adversarial train accuracy: 0.3203125\n",
      "Current adversarial train loss: 3.0661227703094482\n",
      "\n",
      "Current batch: 160\n",
      "Current benign train accuracy: 0.4765625\n",
      "Current benign train loss: 3.174196720123291\n",
      "Current adversarial train accuracy: 0.2734375\n",
      "Current adversarial train loss: 3.616703510284424\n",
      "\n",
      "Current batch: 170\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 2.733581781387329\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.6068055629730225\n",
      "\n",
      "Current batch: 180\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.4884982109069824\n",
      "Current adversarial train accuracy: 0.2421875\n",
      "Current adversarial train loss: 3.111448049545288\n",
      "\n",
      "Current batch: 190\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 1.545945405960083\n",
      "Current adversarial train accuracy: 0.3046875\n",
      "Current adversarial train loss: 3.389915704727173\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 2.1190025806427\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.4992377758026123\n",
      "\n",
      "Current batch: 210\n",
      "Current benign train accuracy: 0.59375\n",
      "Current benign train loss: 3.0264790058135986\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.7729332447052\n",
      "\n",
      "Current batch: 220\n",
      "Current benign train accuracy: 0.1015625\n",
      "Current benign train loss: 3.1289803981781006\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.636387348175049\n",
      "\n",
      "Current batch: 230\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 1.5060813426971436\n",
      "Current adversarial train accuracy: 0.1796875\n",
      "Current adversarial train loss: 3.7986934185028076\n",
      "\n",
      "Current batch: 240\n",
      "Current benign train accuracy: 0.734375\n",
      "Current benign train loss: 2.3890652656555176\n",
      "Current adversarial train accuracy: 0.2890625\n",
      "Current adversarial train loss: 3.5404610633850098\n",
      "\n",
      "Current batch: 250\n",
      "Current benign train accuracy: 0.078125\n",
      "Current benign train loss: 3.031680107116699\n",
      "Current adversarial train accuracy: 0.265625\n",
      "Current adversarial train loss: 3.608297348022461\n",
      "\n",
      "Current batch: 260\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.1466140747070312\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.585287094116211\n",
      "\n",
      "Current batch: 270\n",
      "Current benign train accuracy: 0.7109375\n",
      "Current benign train loss: 2.341400623321533\n",
      "Current adversarial train accuracy: 0.0390625\n",
      "Current adversarial train loss: 3.786055326461792\n",
      "\n",
      "Current batch: 280\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.9213237762451172\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.4663302898406982\n",
      "\n",
      "Current batch: 290\n",
      "Current benign train accuracy: 0.6328125\n",
      "Current benign train loss: 2.776473045349121\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.531229019165039\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 1.8709893226623535\n",
      "Current adversarial train accuracy: 0.25\n",
      "Current adversarial train loss: 3.5439891815185547\n",
      "\n",
      "Current batch: 310\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.2128405570983887\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.0362548828125\n",
      "\n",
      "Current batch: 320\n",
      "Current benign train accuracy: 0.78125\n",
      "Current benign train loss: 1.4666738510131836\n",
      "Current adversarial train accuracy: 0.265625\n",
      "Current adversarial train loss: 3.746236562728882\n",
      "\n",
      "Current batch: 330\n",
      "Current benign train accuracy: 0.46875\n",
      "Current benign train loss: 3.2290091514587402\n",
      "Current adversarial train accuracy: 0.34375\n",
      "Current adversarial train loss: 3.3841118812561035\n",
      "\n",
      "Current batch: 340\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 2.520467758178711\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.8592283725738525\n",
      "\n",
      "Current batch: 350\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 1.1795158386230469\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.456394672393799\n",
      "\n",
      "Current batch: 360\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 2.609622001647949\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.135960340499878\n",
      "\n",
      "Current batch: 370\n",
      "Current benign train accuracy: 0.484375\n",
      "Current benign train loss: 3.140845775604248\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.3891167640686035\n",
      "\n",
      "Current batch: 380\n",
      "Current benign train accuracy: 0.09375\n",
      "Current benign train loss: 3.2385318279266357\n",
      "Current adversarial train accuracy: 0.171875\n",
      "Current adversarial train loss: 3.8006370067596436\n",
      "\n",
      "Current batch: 390\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 2.0251777172088623\n",
      "Current adversarial train accuracy: 0.225\n",
      "Current adversarial train loss: 3.785111904144287\n",
      "\n",
      "Total benign train accuarcy: tensor(52.0865, device='cuda:0')\n",
      "Total adversarial train accuarcy: tensor(19.8784, device='cuda:0')\n",
      "Total benign train loss: 951.6581747531891\n",
      "Total adversarial train loss: 1393.5005807876587\n",
      "\n",
      "[ Test epoch: 7 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign test accuracy: 0.64\n",
      "Current benign test loss: 1.5072747468948364\n",
      "Current adversarial test accuracy: 0.23\n",
      "Current adversarial test loss: 3.552077531814575\n",
      "\n",
      "Current batch: 10\n",
      "Current benign test accuracy: 0.64\n",
      "Current benign test loss: 1.5446209907531738\n",
      "Current adversarial test accuracy: 0.16\n",
      "Current adversarial test loss: 3.7446134090423584\n",
      "\n",
      "Current batch: 20\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.518442988395691\n",
      "Current adversarial test accuracy: 0.19\n",
      "Current adversarial test loss: 3.5770533084869385\n",
      "\n",
      "Current batch: 30\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.7093943357467651\n",
      "Current adversarial test accuracy: 0.16\n",
      "Current adversarial test loss: 3.777928113937378\n",
      "\n",
      "Current batch: 40\n",
      "Current benign test accuracy: 0.65\n",
      "Current benign test loss: 1.5593420267105103\n",
      "Current adversarial test accuracy: 0.21\n",
      "Current adversarial test loss: 3.6974260807037354\n",
      "\n",
      "Current batch: 50\n",
      "Current benign test accuracy: 0.66\n",
      "Current benign test loss: 1.5312414169311523\n",
      "Current adversarial test accuracy: 0.23\n",
      "Current adversarial test loss: 3.5222156047821045\n",
      "\n",
      "Current batch: 60\n",
      "Current benign test accuracy: 0.63\n",
      "Current benign test loss: 1.5990591049194336\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.584451675415039\n",
      "\n",
      "Current batch: 70\n",
      "Current benign test accuracy: 0.64\n",
      "Current benign test loss: 1.5877679586410522\n",
      "Current adversarial test accuracy: 0.16\n",
      "Current adversarial test loss: 3.7353992462158203\n",
      "\n",
      "Current batch: 80\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.6435186862945557\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.6818037033081055\n",
      "\n",
      "Current batch: 90\n",
      "Current benign test accuracy: 0.63\n",
      "Current benign test loss: 1.7337740659713745\n",
      "Current adversarial test accuracy: 0.19\n",
      "Current adversarial test loss: 3.7910406589508057\n",
      "\n",
      "Total benign test accuarcy: 64.72\n",
      "Total adversarial test Accuarcy: 18.58\n",
      "Total benign test loss: 157.5116206407547\n",
      "Total adversarial test loss: 368.4297697544098\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 8 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.15625\n",
      "Current benign train loss: 3.32248592376709\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.3718903064727783\n",
      "\n",
      "Current batch: 10\n",
      "Current benign train accuracy: 0.65625\n",
      "Current benign train loss: 2.8284637928009033\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.862734317779541\n",
      "\n",
      "Current batch: 20\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 2.545966625213623\n",
      "Current adversarial train accuracy: 0.0546875\n",
      "Current adversarial train loss: 3.759222984313965\n",
      "\n",
      "Current batch: 30\n",
      "Current benign train accuracy: 0.203125\n",
      "Current benign train loss: 3.4107794761657715\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.5407776832580566\n",
      "\n",
      "Current batch: 40\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.8665833473205566\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.5720343589782715\n",
      "\n",
      "Current batch: 50\n",
      "Current benign train accuracy: 0.703125\n",
      "Current benign train loss: 2.6416401863098145\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.1600234508514404\n",
      "\n",
      "Current batch: 60\n",
      "Current benign train accuracy: 0.109375\n",
      "Current benign train loss: 3.1727778911590576\n",
      "Current adversarial train accuracy: 0.2109375\n",
      "Current adversarial train loss: 3.666491985321045\n",
      "\n",
      "Current batch: 70\n",
      "Current benign train accuracy: 0.7734375\n",
      "Current benign train loss: 1.4547126293182373\n",
      "Current adversarial train accuracy: 0.1875\n",
      "Current adversarial train loss: 3.794424533843994\n",
      "\n",
      "Current batch: 80\n",
      "Current benign train accuracy: 0.6328125\n",
      "Current benign train loss: 2.7992773056030273\n",
      "Current adversarial train accuracy: 0.25\n",
      "Current adversarial train loss: 3.8074541091918945\n",
      "\n",
      "Current batch: 90\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 3.1787891387939453\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 3.5408642292022705\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.2035472393035889\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.4221549034118652\n",
      "\n",
      "Current batch: 110\n",
      "Current benign train accuracy: 0.71875\n",
      "Current benign train loss: 2.314384937286377\n",
      "Current adversarial train accuracy: 0.0859375\n",
      "Current adversarial train loss: 3.8047566413879395\n",
      "\n",
      "Current batch: 120\n",
      "Current benign train accuracy: 0.390625\n",
      "Current benign train loss: 3.3125391006469727\n",
      "Current adversarial train accuracy: 0.2265625\n",
      "Current adversarial train loss: 3.734611749649048\n",
      "\n",
      "Current batch: 130\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 2.6964149475097656\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.476499080657959\n",
      "\n",
      "Current batch: 140\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 1.3624404668807983\n",
      "Current adversarial train accuracy: 0.09375\n",
      "Current adversarial train loss: 3.7232909202575684\n",
      "\n",
      "Current batch: 150\n",
      "Current benign train accuracy: 0.6796875\n",
      "Current benign train loss: 2.363626480102539\n",
      "Current adversarial train accuracy: 0.1171875\n",
      "Current adversarial train loss: 3.9091849327087402\n",
      "\n",
      "Current batch: 160\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 2.124962329864502\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.7569804191589355\n",
      "\n",
      "Current batch: 170\n",
      "Current benign train accuracy: 0.75\n",
      "Current benign train loss: 2.393216133117676\n",
      "Current adversarial train accuracy: 0.3046875\n",
      "Current adversarial train loss: 3.6054582595825195\n",
      "\n",
      "Current batch: 180\n",
      "Current benign train accuracy: 0.78125\n",
      "Current benign train loss: 1.816755771636963\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.01410174369812\n",
      "\n",
      "Current batch: 190\n",
      "Current benign train accuracy: 0.1328125\n",
      "Current benign train loss: 3.1869020462036133\n",
      "Current adversarial train accuracy: 0.1953125\n",
      "Current adversarial train loss: 3.7820281982421875\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.4268436431884766\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.5218043327331543\n",
      "\n",
      "Current batch: 210\n",
      "Current benign train accuracy: 0.1640625\n",
      "Current benign train loss: 3.3119046688079834\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.4757208824157715\n",
      "\n",
      "Current batch: 220\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 2.7002649307250977\n",
      "Current adversarial train accuracy: 0.296875\n",
      "Current adversarial train loss: 3.2599267959594727\n",
      "\n",
      "Current batch: 230\n",
      "Current benign train accuracy: 0.640625\n",
      "Current benign train loss: 2.848172187805176\n",
      "Current adversarial train accuracy: 0.203125\n",
      "Current adversarial train loss: 3.7071845531463623\n",
      "\n",
      "Current batch: 240\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 2.4255542755126953\n",
      "Current adversarial train accuracy: 0.078125\n",
      "Current adversarial train loss: 3.7165908813476562\n",
      "\n",
      "Current batch: 250\n",
      "Current benign train accuracy: 0.2734375\n",
      "Current benign train loss: 3.486180543899536\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.595116138458252\n",
      "\n",
      "Current batch: 260\n",
      "Current benign train accuracy: 0.734375\n",
      "Current benign train loss: 2.535839080810547\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.7452330589294434\n",
      "\n",
      "Current batch: 270\n",
      "Current benign train accuracy: 0.4921875\n",
      "Current benign train loss: 3.2660632133483887\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 3.7873353958129883\n",
      "\n",
      "Current batch: 280\n",
      "Current benign train accuracy: 0.3359375\n",
      "Current benign train loss: 3.3182566165924072\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.2642228603363037\n",
      "\n",
      "Current batch: 290\n",
      "Current benign train accuracy: 0.375\n",
      "Current benign train loss: 3.284249782562256\n",
      "Current adversarial train accuracy: 0.078125\n",
      "Current adversarial train loss: 3.7368884086608887\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.4609375\n",
      "Current benign train loss: 3.167409658432007\n",
      "Current adversarial train accuracy: 0.3125\n",
      "Current adversarial train loss: 3.5045320987701416\n",
      "\n",
      "Current batch: 310\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 2.914999008178711\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.2520363330841064\n",
      "\n",
      "Current batch: 320\n",
      "Current benign train accuracy: 0.1328125\n",
      "Current benign train loss: 3.3492839336395264\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.766265869140625\n",
      "\n",
      "Current batch: 330\n",
      "Current benign train accuracy: 0.0234375\n",
      "Current benign train loss: 1.3766714334487915\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.462979793548584\n",
      "\n",
      "Current batch: 340\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 1.349162220954895\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.7881879806518555\n",
      "\n",
      "Current batch: 350\n",
      "Current benign train accuracy: 0.5859375\n",
      "Current benign train loss: 3.157581329345703\n",
      "Current adversarial train accuracy: 0.046875\n",
      "Current adversarial train loss: 3.64963960647583\n",
      "\n",
      "Current batch: 360\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 2.1539056301116943\n",
      "Current adversarial train accuracy: 0.265625\n",
      "Current adversarial train loss: 3.8210504055023193\n",
      "\n",
      "Current batch: 370\n",
      "Current benign train accuracy: 0.671875\n",
      "Current benign train loss: 2.7538204193115234\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.7461442947387695\n",
      "\n",
      "Current batch: 380\n",
      "Current benign train accuracy: 0.1875\n",
      "Current benign train loss: 3.27016282081604\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.376607656478882\n",
      "\n",
      "Current batch: 390\n",
      "Current benign train accuracy: 0.075\n",
      "Current benign train loss: 3.0849075317382812\n",
      "Current adversarial train accuracy: 0.025\n",
      "Current adversarial train loss: 3.048870086669922\n",
      "\n",
      "Total benign train accuarcy: tensor(50.1159, device='cuda:0')\n",
      "Total adversarial train accuarcy: tensor(20.2529, device='cuda:0')\n",
      "Total benign train loss: 976.6238561868668\n",
      "Total adversarial train loss: 1387.2451438903809\n",
      "\n",
      "[ Test epoch: 8 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign test accuracy: 0.66\n",
      "Current benign test loss: 1.4814964532852173\n",
      "Current adversarial test accuracy: 0.25\n",
      "Current adversarial test loss: 3.4827234745025635\n",
      "\n",
      "Current batch: 10\n",
      "Current benign test accuracy: 0.64\n",
      "Current benign test loss: 1.4990403652191162\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.670788288116455\n",
      "\n",
      "Current batch: 20\n",
      "Current benign test accuracy: 0.63\n",
      "Current benign test loss: 1.4868137836456299\n",
      "Current adversarial test accuracy: 0.19\n",
      "Current adversarial test loss: 3.5239803791046143\n",
      "\n",
      "Current batch: 30\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.6857032775878906\n",
      "Current adversarial test accuracy: 0.15\n",
      "Current adversarial test loss: 3.735114336013794\n",
      "\n",
      "Current batch: 40\n",
      "Current benign test accuracy: 0.68\n",
      "Current benign test loss: 1.5155994892120361\n",
      "Current adversarial test accuracy: 0.21\n",
      "Current adversarial test loss: 3.6329925060272217\n",
      "\n",
      "Current batch: 50\n",
      "Current benign test accuracy: 0.66\n",
      "Current benign test loss: 1.5057755708694458\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.463848352432251\n",
      "\n",
      "Current batch: 60\n",
      "Current benign test accuracy: 0.64\n",
      "Current benign test loss: 1.5740197896957397\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.5259287357330322\n",
      "\n",
      "Current batch: 70\n",
      "Current benign test accuracy: 0.63\n",
      "Current benign test loss: 1.5510364770889282\n",
      "Current adversarial test accuracy: 0.15\n",
      "Current adversarial test loss: 3.6795363426208496\n",
      "\n",
      "Current batch: 80\n",
      "Current benign test accuracy: 0.62\n",
      "Current benign test loss: 1.6109991073608398\n",
      "Current adversarial test accuracy: 0.21\n",
      "Current adversarial test loss: 3.611349582672119\n",
      "\n",
      "Current batch: 90\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.7172257900238037\n",
      "Current adversarial test accuracy: 0.18\n",
      "Current adversarial test loss: 3.7481493949890137\n",
      "\n",
      "Total benign test accuarcy: 65.07\n",
      "Total adversarial test Accuarcy: 19.49\n",
      "Total benign test loss: 154.58744025230408\n",
      "Total adversarial test loss: 361.87326407432556\n",
      "Model Saved!\n",
      "\n",
      "[ Train epoch: 9 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.3790396451950073\n",
      "Current adversarial train accuracy: 0.2734375\n",
      "Current adversarial train loss: 3.191915988922119\n",
      "\n",
      "Current batch: 10\n",
      "Current benign train accuracy: 0.71875\n",
      "Current benign train loss: 2.4774951934814453\n",
      "Current adversarial train accuracy: 0.140625\n",
      "Current adversarial train loss: 3.8767268657684326\n",
      "\n",
      "Current batch: 20\n",
      "Current benign train accuracy: 0.6015625\n",
      "Current benign train loss: 3.0087227821350098\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.371534824371338\n",
      "\n",
      "Current batch: 30\n",
      "Current benign train accuracy: 0.765625\n",
      "Current benign train loss: 2.444417953491211\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.8616809844970703\n",
      "\n",
      "Current batch: 40\n",
      "Current benign train accuracy: 0.7421875\n",
      "Current benign train loss: 2.234666585922241\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.6750802993774414\n",
      "\n",
      "Current batch: 50\n",
      "Current benign train accuracy: 0.8046875\n",
      "Current benign train loss: 1.7933475971221924\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.369427442550659\n",
      "\n",
      "Current batch: 60\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 1.693222999572754\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.2695844173431396\n",
      "\n",
      "Current batch: 70\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 3.010441541671753\n",
      "Current adversarial train accuracy: 0.234375\n",
      "Current adversarial train loss: 3.3992271423339844\n",
      "\n",
      "Current batch: 80\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 1.8401377201080322\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.765781879425049\n",
      "\n",
      "Current batch: 90\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 2.3821487426757812\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.3175716400146484\n",
      "\n",
      "Current batch: 100\n",
      "Current benign train accuracy: 0.5625\n",
      "Current benign train loss: 2.991440773010254\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.238037109375\n",
      "\n",
      "Current batch: 110\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 2.39023494720459\n",
      "Current adversarial train accuracy: 0.265625\n",
      "Current adversarial train loss: 3.520948886871338\n",
      "\n",
      "Current batch: 120\n",
      "Current benign train accuracy: 0.75\n",
      "Current benign train loss: 2.4449281692504883\n",
      "Current adversarial train accuracy: 0.296875\n",
      "Current adversarial train loss: 3.6167826652526855\n",
      "\n",
      "Current batch: 130\n",
      "Current benign train accuracy: 0.6953125\n",
      "Current benign train loss: 2.5182723999023438\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.828577756881714\n",
      "\n",
      "Current batch: 140\n",
      "Current benign train accuracy: 0.1484375\n",
      "Current benign train loss: 3.05900502204895\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.6358163356781006\n",
      "\n",
      "Current batch: 150\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 3.002974510192871\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.6214380264282227\n",
      "\n",
      "Current batch: 160\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 2.3472554683685303\n",
      "Current adversarial train accuracy: 0.078125\n",
      "Current adversarial train loss: 3.8452274799346924\n",
      "\n",
      "Current batch: 170\n",
      "Current benign train accuracy: 0.1875\n",
      "Current benign train loss: 3.2644081115722656\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.6144979000091553\n",
      "\n",
      "Current batch: 180\n",
      "Current benign train accuracy: 0.8515625\n",
      "Current benign train loss: 1.3130989074707031\n",
      "Current adversarial train accuracy: 0.3125\n",
      "Current adversarial train loss: 3.1393399238586426\n",
      "\n",
      "Current batch: 190\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.8499051332473755\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.087688446044922\n",
      "\n",
      "Current batch: 200\n",
      "Current benign train accuracy: 0.3828125\n",
      "Current benign train loss: 3.3284435272216797\n",
      "Current adversarial train accuracy: 0.2734375\n",
      "Current adversarial train loss: 3.561342239379883\n",
      "\n",
      "Current batch: 210\n",
      "Current benign train accuracy: 0.7890625\n",
      "Current benign train loss: 2.3046464920043945\n",
      "Current adversarial train accuracy: 0.0078125\n",
      "Current adversarial train loss: 3.5840723514556885\n",
      "\n",
      "Current batch: 220\n",
      "Current benign train accuracy: 0.40625\n",
      "Current benign train loss: 3.4000086784362793\n",
      "Current adversarial train accuracy: 0.0\n",
      "Current adversarial train loss: 3.330662250518799\n",
      "\n",
      "Current batch: 230\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 2.501181125640869\n",
      "Current adversarial train accuracy: 0.140625\n",
      "Current adversarial train loss: 3.779756784439087\n",
      "\n",
      "Current batch: 240\n",
      "Current benign train accuracy: 0.703125\n",
      "Current benign train loss: 2.5706586837768555\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.3641843795776367\n",
      "\n",
      "Current batch: 250\n",
      "Current benign train accuracy: 0.8359375\n",
      "Current benign train loss: 1.3699393272399902\n",
      "Current adversarial train accuracy: 0.1953125\n",
      "Current adversarial train loss: 3.7348289489746094\n",
      "\n",
      "Current batch: 260\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 2.016845703125\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.6079330444335938\n",
      "\n",
      "Current batch: 270\n",
      "Current benign train accuracy: 0.0078125\n",
      "Current benign train loss: 1.8672086000442505\n",
      "Current adversarial train accuracy: 0.1953125\n",
      "Current adversarial train loss: 3.9615659713745117\n",
      "\n",
      "Current batch: 280\n",
      "Current benign train accuracy: 0.328125\n",
      "Current benign train loss: 3.2743537425994873\n",
      "Current adversarial train accuracy: 0.0625\n",
      "Current adversarial train loss: 3.756857395172119\n",
      "\n",
      "Current batch: 290\n",
      "Current benign train accuracy: 0.0390625\n",
      "Current benign train loss: 2.5592262744903564\n",
      "Current adversarial train accuracy: 0.15625\n",
      "Current adversarial train loss: 3.7945005893707275\n",
      "\n",
      "Current batch: 300\n",
      "Current benign train accuracy: 0.6875\n",
      "Current benign train loss: 2.1570746898651123\n",
      "Current adversarial train accuracy: 0.015625\n",
      "Current adversarial train loss: 3.3683979511260986\n",
      "\n",
      "Current batch: 310\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 2.814457416534424\n",
      "Current adversarial train accuracy: 0.03125\n",
      "Current adversarial train loss: 3.081509828567505\n",
      "\n",
      "Current batch: 320\n",
      "Current benign train accuracy: 0.046875\n",
      "Current benign train loss: 1.9324718713760376\n",
      "Current adversarial train accuracy: 0.21875\n",
      "Current adversarial train loss: 3.848574161529541\n",
      "\n",
      "Current batch: 330\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 1.5891759395599365\n",
      "Current adversarial train accuracy: 0.34375\n",
      "Current adversarial train loss: 3.0354578495025635\n",
      "\n",
      "Current batch: 340\n",
      "Current benign train accuracy: 0.7734375\n",
      "Current benign train loss: 1.2055835723876953\n",
      "Current adversarial train accuracy: 0.0234375\n",
      "Current adversarial train loss: 3.337596893310547\n",
      "\n",
      "Current batch: 350\n",
      "Current benign train accuracy: 0.71875\n",
      "Current benign train loss: 2.348177909851074\n",
      "Current adversarial train accuracy: 0.09375\n",
      "Current adversarial train loss: 3.6219184398651123\n",
      "\n",
      "Current batch: 360\n",
      "Current benign train accuracy: 0.03125\n",
      "Current benign train loss: 2.0349526405334473\n",
      "Current adversarial train accuracy: 0.1875\n",
      "Current adversarial train loss: 3.267599105834961\n",
      "\n",
      "Current batch: 370\n",
      "Current benign train accuracy: 0.015625\n",
      "Current benign train loss: 1.9054889678955078\n",
      "Current adversarial train accuracy: 0.2578125\n",
      "Current adversarial train loss: 3.58809494972229\n",
      "\n",
      "Current batch: 380\n",
      "Current benign train accuracy: 0.0\n",
      "Current benign train loss: 1.745118498802185\n",
      "Current adversarial train accuracy: 0.28125\n",
      "Current adversarial train loss: 3.170522451400757\n",
      "\n",
      "Current batch: 390\n",
      "Current benign train accuracy: 0.0875\n",
      "Current benign train loss: 3.116799831390381\n",
      "Current adversarial train accuracy: 0.3125\n",
      "Current adversarial train loss: 3.546215057373047\n",
      "\n",
      "Total benign train accuarcy: tensor(52.3304, device='cuda:0')\n",
      "Total adversarial train accuarcy: tensor(20.4300, device='cuda:0')\n",
      "Total benign train loss: 948.498059630394\n",
      "Total adversarial train loss: 1383.2191927433014\n",
      "\n",
      "[ Test epoch: 9 ]\n",
      "\n",
      "Current batch: 0\n",
      "Current benign test accuracy: 0.7\n",
      "Current benign test loss: 1.4376537799835205\n",
      "Current adversarial test accuracy: 0.27\n",
      "Current adversarial test loss: 3.5380337238311768\n",
      "\n",
      "Current batch: 10\n",
      "Current benign test accuracy: 0.66\n",
      "Current benign test loss: 1.4768517017364502\n",
      "Current adversarial test accuracy: 0.14\n",
      "Current adversarial test loss: 3.729485511779785\n",
      "\n",
      "Current batch: 20\n",
      "Current benign test accuracy: 0.63\n",
      "Current benign test loss: 1.4710595607757568\n",
      "Current adversarial test accuracy: 0.2\n",
      "Current adversarial test loss: 3.5814523696899414\n",
      "\n",
      "Current batch: 30\n",
      "Current benign test accuracy: 0.6\n",
      "Current benign test loss: 1.641446828842163\n",
      "Current adversarial test accuracy: 0.16\n",
      "Current adversarial test loss: 3.80722713470459\n",
      "\n",
      "Current batch: 40\n",
      "Current benign test accuracy: 0.69\n",
      "Current benign test loss: 1.4664939641952515\n",
      "Current adversarial test accuracy: 0.19\n",
      "Current adversarial test loss: 3.695589542388916\n",
      "\n",
      "Current batch: 50\n",
      "Current benign test accuracy: 0.66\n",
      "Current benign test loss: 1.4295556545257568\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.46462345123291\n",
      "\n",
      "Current batch: 60\n",
      "Current benign test accuracy: 0.64\n",
      "Current benign test loss: 1.5260673761367798\n",
      "Current adversarial test accuracy: 0.23\n",
      "Current adversarial test loss: 3.557403326034546\n",
      "\n",
      "Current batch: 70\n",
      "Current benign test accuracy: 0.65\n",
      "Current benign test loss: 1.4982434511184692\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.699246883392334\n",
      "\n",
      "Current batch: 80\n",
      "Current benign test accuracy: 0.61\n",
      "Current benign test loss: 1.5944827795028687\n",
      "Current adversarial test accuracy: 0.22\n",
      "Current adversarial test loss: 3.7151732444763184\n",
      "\n",
      "Current batch: 90\n",
      "Current benign test accuracy: 0.58\n",
      "Current benign test loss: 1.6798744201660156\n",
      "Current adversarial test accuracy: 0.17\n",
      "Current adversarial test loss: 3.804426908493042\n",
      "\n",
      "Total benign test accuarcy: 64.96\n",
      "Total adversarial test Accuarcy: 19.32\n",
      "Total benign test loss: 149.51804864406586\n",
      "Total adversarial test loss: 366.19128131866455\n",
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#interpolated_adversarial_training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#from models import *\n",
    "\n",
    "learning_rate = 0.1\n",
    "epsilon = 0.0314\n",
    "k = 7\n",
    "alpha = 0.00784\n",
    "file_name = 'interpolated_adversarial_training'\n",
    "mixup_alpha = 1.0\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)\n",
    "\n",
    "def mixup_data(x, y):\n",
    "    lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "class LinfPGDAttack(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def perturb(self, x_natural, y):\n",
    "        x = x_natural.detach()\n",
    "        x = x + torch.zeros_like(x).uniform_(-8/225, 8/225)\n",
    "        for i in range(k):\n",
    "            x.requires_grad_()\n",
    "            with torch.enable_grad():\n",
    "                logits = self.model(x)\n",
    "                loss = F.cross_entropy(logits, y)\n",
    "            grad = torch.autograd.grad(loss, [x])[0]\n",
    "            x = x.detach() + alpha * torch.sign(grad.detach())\n",
    "            x = torch.min(torch.max(x, x_natural - 8/225), x_natural + 8/225)\n",
    "            x = torch.clamp(x, 0, 1)\n",
    "        return x\n",
    "\n",
    "def attack(x, y, model, adversary):\n",
    "    model_copied = copy.deepcopy(model)\n",
    "    model_copied.eval()\n",
    "    adversary.model = model_copied\n",
    "    adv = adversary.perturb(x, y)\n",
    "    return adv\n",
    "\n",
    "adversary = LinfPGDAttack(net)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n",
    "\n",
    "def train(epoch):\n",
    "    print('\\n[ Train epoch: %d ]' % epoch)\n",
    "    net.train()\n",
    "    benign_loss = 0\n",
    "    adv_loss = 0\n",
    "    benign_correct = 0\n",
    "    adv_correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        total += targets.size(0)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        benign_inputs, benign_targets_a, benign_targets_b, benign_lam = mixup_data(inputs, targets)\n",
    "        benign_outputs = net(benign_inputs)\n",
    "        loss1 = mixup_criterion(criterion, benign_outputs, benign_targets_a, benign_targets_b, benign_lam)\n",
    "        benign_loss += loss1.item()\n",
    "\n",
    "        _, predicted = benign_outputs.max(1)\n",
    "        benign_correct += (benign_lam * predicted.eq(benign_targets_a).sum().float() + (1 - benign_lam) * predicted.eq(benign_targets_b).sum().float())\n",
    "        if batch_idx % 10 == 0:\n",
    "                print('\\nCurrent batch:', str(batch_idx))\n",
    "                print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "                print('Current benign train loss:', loss1.item())\n",
    "\n",
    "        adv = adversary.perturb(inputs, targets)\n",
    "        adv_inputs, adv_targets_a, adv_targets_b, adv_lam = mixup_data(adv, targets)\n",
    "        adv_outputs = net(adv_inputs)\n",
    "        loss2 = mixup_criterion(criterion, adv_outputs, adv_targets_a, adv_targets_b, adv_lam)\n",
    "        adv_loss += loss2.item()\n",
    "\n",
    "        _, predicted = adv_outputs.max(1)\n",
    "        adv_correct += (adv_lam * predicted.eq(adv_targets_a).sum().float() + (1 - adv_lam) * predicted.eq(adv_targets_b).sum().float())\n",
    "        if batch_idx % 10 == 0:\n",
    "                print('Current adversarial train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "                print('Current adversarial train loss:', loss2.item())\n",
    "\n",
    "        loss = (loss1 + loss2) / 2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('\\nTotal benign train accuarcy:', 100. * benign_correct / total)\n",
    "    print('Total adversarial train accuarcy:', 100. * adv_correct / total)\n",
    "    print('Total benign train loss:', benign_loss)\n",
    "    print('Total adversarial train loss:', adv_loss)\n",
    "\n",
    "def test(epoch):\n",
    "    print('\\n[ Test epoch: %d ]' % epoch)\n",
    "    net.eval()\n",
    "    benign_loss = 0\n",
    "    adv_loss = 0\n",
    "    benign_correct = 0\n",
    "    adv_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            total += targets.size(0)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            benign_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            benign_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('\\nCurrent batch:', str(batch_idx))\n",
    "                print('Current benign test accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "                print('Current benign test loss:', loss.item())\n",
    "\n",
    "            adv = adversary.perturb(inputs, targets)\n",
    "            adv_outputs = net(adv)\n",
    "            loss = criterion(adv_outputs, targets)\n",
    "            adv_loss += loss.item()\n",
    "\n",
    "            _, predicted = adv_outputs.max(1)\n",
    "            adv_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print('Current adversarial test accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
    "                print('Current adversarial test loss:', loss.item())\n",
    "\n",
    "    print('\\nTotal benign test accuarcy:', 100. * benign_correct / total)\n",
    "    print('Total adversarial test Accuarcy:', 100. * adv_correct / total)\n",
    "    print('Total benign test loss:', benign_loss)\n",
    "    print('Total adversarial test loss:', adv_loss)\n",
    "\n",
    "    state = {\n",
    "        'net': net.state_dict()\n",
    "    }\n",
    "    if not os.path.isdir('checkpoint'):\n",
    "        os.mkdir('checkpoint')\n",
    "    torch.save(state, './checkpoint/' + file_name)\n",
    "    print('Model Saved!')\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = learning_rate\n",
    "    if epoch >= 5:\n",
    "        lr /= 100\n",
    "    if epoch >= 50:\n",
    "        lr /= 10\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zpwh2LSnD8YP"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1, 3, 32, 32))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "yp_ojLzYD_4A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "net = net.to(device)\n",
    "cudnn.benchmark = True\n",
    "checkpoint = torch.load('./checkpoint/' + file_name)\n",
    "net.load_state_dict(checkpoint['net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZMM0DC7OEBX3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "l = [x for (x, y) in test_loader]\n",
    "x_test = torch.cat(l, 0)\n",
    "l = [y for (x, y) in test_loader]\n",
    "y_test = torch.cat(l, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "f5-qHLRyEDHn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zahra\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Average loss: 1.4952, Accuracy: 6496/10000 (65%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.495180485534668, 0.6496)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n",
    "eval_test(net, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "s8bCZwQdEFGo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/fra31/auto-attack\n",
      "  Cloning https://github.com/fra31/auto-attack to c:\\users\\zahra\\appdata\\local\\temp\\pip-req-build-xkvd31d0\n",
      "  Resolved https://github.com/fra31/auto-attack to commit a39220048b3c9f2cca9a4d3a54604793c68eca7e\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/fra31/auto-attack 'C:\\Users\\zahra\\AppData\\Local\\Temp\\pip-req-build-xkvd31d0'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/fra31/auto-attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "s2PYpXYaEIdI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting parameters for standard version\n"
     ]
    }
   ],
   "source": [
    "from autoattack import AutoAttack\n",
    "adversary = AutoAttack(net, norm='Linf', eps=0.031, version='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ArUVo3BgEI4I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using standard version including apgd-ce, apgd-t, fab-t, square.\n",
      "initial accuracy: 64.96%\n",
      "apgd-ce - 1/13 - 359 out of 500 successfully perturbed\n",
      "apgd-ce - 2/13 - 368 out of 500 successfully perturbed\n",
      "apgd-ce - 3/13 - 361 out of 500 successfully perturbed\n",
      "apgd-ce - 4/13 - 372 out of 500 successfully perturbed\n",
      "apgd-ce - 5/13 - 351 out of 500 successfully perturbed\n",
      "apgd-ce - 6/13 - 364 out of 500 successfully perturbed\n",
      "apgd-ce - 7/13 - 364 out of 500 successfully perturbed\n",
      "apgd-ce - 8/13 - 357 out of 500 successfully perturbed\n",
      "apgd-ce - 9/13 - 354 out of 500 successfully perturbed\n",
      "apgd-ce - 10/13 - 368 out of 500 successfully perturbed\n",
      "apgd-ce - 11/13 - 371 out of 500 successfully perturbed\n",
      "apgd-ce - 12/13 - 352 out of 500 successfully perturbed\n",
      "apgd-ce - 13/13 - 350 out of 496 successfully perturbed\n",
      "robust accuracy after APGD-CE: 18.05% (total time 171.8 s)\n"
     ]
    }
   ],
   "source": [
    "adv_complete = adversary.run_standard_evaluation(x_test, y_test,bs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a8HDRWSuEKWg"
   },
   "outputs": [],
   "source": [
    "adv_loader_Linf = torch.utils.data.DataLoader(adv_complete, batch_size=100, shuffle=False, num_workers=2)\n",
    "num_total_images = 0\n",
    "num_successful_attacks = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    for adv_images in adv_loader_Linf:\n",
    "        adv_images = adv_images.to(device)\n",
    "        outputs_adv = net(adv_images)\n",
    "        _, predicted_adv = torch.max(outputs_adv.data, 1)\n",
    "\n",
    "        # Count the number of adversarial examples that were successfully attacked\n",
    "        for i in range(len(images)):\n",
    "            if predicted[i] != predicted_adv[i]:\n",
    "                num_successful_attacks += 1\n",
    "        \n",
    "        # Increment the total number of images\n",
    "        num_total_images += len(images)\n",
    "\n",
    "# Calculate the adversarial attack rate\n",
    "attack_rate = (num_successful_attacks / num_total_images) * 100\n",
    "print(f\"Adversarial attack rate: {attack_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofQG7WwvEN_3"
   },
   "outputs": [],
   "source": [
    "from autoattack import AutoAttack\n",
    "adversary_L2 = AutoAttack(net, norm='L2', eps=0.031, version='standard')\n",
    "adv_complete_L2 = adversary_L2.run_standard_evaluation(x_test, y_test,bs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qQLj9poXEQgX"
   },
   "outputs": [],
   "source": [
    "adv_loader_L2 = torch.utils.data.DataLoader(adv_complete_L2, batch_size=100, shuffle=False, num_workers=2)\n",
    "num_total_images = 0\n",
    "num_successful_attacks = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    for adv_images in adv_loader_L2:\n",
    "        adv_images = adv_images.to(device)\n",
    "        outputs_adv = net(adv_images)\n",
    "        _, predicted_adv = torch.max(outputs_adv.data, 1)\n",
    "\n",
    "        # Count the number of adversarial examples that were successfully attacked\n",
    "        for i in range(len(images)):\n",
    "            if predicted[i] != predicted_adv[i]:\n",
    "                num_successful_attacks += 1\n",
    "        \n",
    "        # Increment the total number of images\n",
    "        num_total_images += len(images)\n",
    "\n",
    "# Calculate the adversarial attack rate\n",
    "attack_rate = (num_successful_attacks / num_total_images) * 100\n",
    "print(f\"Adversarial attack rate: {attack_rate:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
